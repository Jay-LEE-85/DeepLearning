[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DeepLearning x 101",
    "section": "",
    "text": "Preface",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "DeepLearning x 101",
    "section": "Welcome",
    "text": "Welcome\n최근 전세계적으로 AI에 대한 관심이 증가하고 있습니다. 특히 OPEN AI의 Chat-GPT를 시작으로 다양한 생성형 AI가 발표되고 있습니다다.\n이러한 AI 발전의 기초가되는 기술은 Machinelearning과 Deeplearning이 있습니다. 이중 가장 기초가 되는 Deeplearning을 활용하여 Data에서 사람이 찾을 수 없는 Features와 Patterns을 기계를 활용하여 찾기 위한 기술을 학습하고자 합니다.\n이 사이트는 앞서 말한바와 같이 날로 중요해지는 Deeplearning에 대한 기초적인 이해부터 구현까지 학습하는 과정에 대한 기록물입니다. 나아가 Deeplearning기술을 활용하여 금융분야에 활용할 수 있는 방법을 연구하거나, 현재 다양한 금융공학의 기술과 접목시키고자 합니다.\n또한, Deeplearning과 관련된 사항들 수식과 이론에 기반하여 기초적인 사항들을 가급적 빠짐없이 다룰 예정입니다. 또한, Deeplearning과 관련된 내용을 ’engineering’관점에서 실제 기능을 구현하기 위한 코드를 포함할 예정이며, 이러한 소스 코드를 통해 이해의 폭을 높이고자 합니다. 물론 모든 코드와 그에 따른 결과는 매 페이지에서 확인할 수 있습니다.\n\n\n\n\n\n\nNote\n\n\n\n본 사이트는 Quarto를 기반으로 작성되었으며 실습환경에 관하여는 Table 1.1 을 참고하기 바랍니다. 본 사이트 구축에 관한 소스 코드는 GitHub에서 확인할 수 있습니다.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#disclaimer",
    "href": "index.html#disclaimer",
    "title": "DeepLearning x 101",
    "section": "Disclaimer",
    "text": "Disclaimer\n이 사이트는 모두에게 무료이며, “Deep Learning from Scratch”를 기반으로 학습자들이 연구한 내용을 담고 있습니다. 모든 내용에 대하여 출처를 밝힐 예정이나, 간혹 출처가 빠져 있는 경우 지속적으로 업데이트 하여, 원작자들의 권리를 침해하지 않고 오류를 수정해 나갈 것입니다.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 Learning Path\n필자들의 DL 학습배경에 따라 아래의 경로로 DL을 학습할 계획이며 많은 문헌에서 다루고 있는 학습경로 입니다.\n학습경로가 정확한지? 적절한지? 알 수 없지만, 적어도 A to Z의 관점에서 빠짐없이 모든 내용을 학습해보기로 하였습니다.\n학습경로에 관한 내용은 Figure 1.1 을 참고하기 바라며, 학습을 수행하는 과정마다 변경되거나 추가되는 사항은 지속적으로 반영해 나갈 예정입니다.\nflowchart LR\n  opt[Optimazation]:::ch --&gt; opt1[Solving Problems]\n  opt --&gt; opt2[Gradient Descent] \n\n  opt1 --&gt; opt_l{{Lab1: Linear Regression}}\n  opt2 --&gt; opt_l:::lab\n\n  per[Perceptron]:::ch --&gt; per1[Classification]\n  per --&gt; per2[Logistic Regression]\n\n  per1 --&gt; per_l1{{Lab2: Classification}}:::lab\n  per2 --&gt; per_l2{{Lab2: Logistic Regression}}:::lab\n\n  %% ann[ANN]:::ch --&gt; ann1[Multi Layer Perceptron] --&gt; ann_l{{Lab3:}}:::lab\n  %% ann --&gt; ann2[Artificial NN?]\n\n  %% ann2 --&gt; ann2_1[Recursive Algorithm]\n  %% ann2 --&gt; ann2_2[Dynamic Programing]\n  %% ann2 --&gt; ann2_3[Neural Networks]\n  \n  %% ann2_3 --&gt; ann2_3_3[Loss Function]\n  %% ann2_3 --&gt; ann2_3_1[Optimization]\n  %% ann2_3 --&gt; ann2_3_2[Activation Function]\n\n  %% ann2_1 & ann2_2 & ann2_3_1 & ann2_3_2 & ann2_3_3 --&gt; ann_l{{Lab3: MNIST Classification}}:::lab\n  \n  %% cnn[Convolution-NN]:::ch --&gt; cnn1[NA]\n\n  %% rnn[Recurrent-NN]:::ch --&gt; rnn[NA]\n\n  classDef ch fill:#ccccff\n  classDef lab fill:#ccffcc\n\n\n\n\nFigure 1.1: Learning Path",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#environment",
    "href": "intro.html#environment",
    "title": "1  Introduction",
    "section": "1.2 Environment",
    "text": "1.2 Environment\n우리가 학습하며 사용한 실습환경에 대하여 간단하게 소개하겠습니다.\n누구나 접근이 가능한 Python을 기반으로 하고 있고, 실습에 사용하는 라이브러리(Table 1.1 )는 의존도를 최소화 하기 위하여 numpy를 주로 사용하였습니다. 그리고 실습결과를 도식화하기 위하여는 matplotlib을 사용하였습니다.\n이론에 대한 충분한 실습을 완료한 뒤에는 tensorflow 또는 torch를 사용하기로 하였습니다. 이는 NVIDIA의 GPU를 활용하여 보다 Deep한 신경망을 구현하기 위함임을 참고하여 주시고 학습과정에서 본 Framework의 사용은 최소화 할 예정입니다.\n학습경로와 마찬가지로 아래의 테이블에 적시된 라이브러리와 그 버전은 수시로 업데이트 할 예정입니다.\n\n\n\nTable 1.1: List of Packages\n\n\n\n\n\nName\nVersion\n\n\n\n\nnumpy\n#.#.#\n\n\nmatplotlib\n#.#.#\n\n\ntensorflow\n#.#.#\n\n\ntorch\n#.#.#",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#how-to-read",
    "href": "intro.html#how-to-read",
    "title": "1  Introduction",
    "section": "1.3 How to read",
    "text": "1.3 How to read\n이 사이트에서 다루는 내용은 기본적인 이론에 대한 설명과 이와 관련된 코드와 그 실행 결과 들을 보여줄 것입니다.\nCode Example\n\n기본적으로 코드는 아래의 Code Block에서 모든 내용을 표시하였습니다.\n특별히 중요하거나 추가적인 설명이 필요한 경우 Code Annotation에 표시하였습니다.\n\n\n\n\nListing 1.1: Code Block\n\n\n\ndef add(num1, num2):\n1  result = num1 + num2\n  return  result\n\n\n1\n\nnum1과 num2를 더하여 result에 할당\n\n\n\n\n\n\n\nEquation Example\n\n수식 중 설명이 필요한 경우는 기본적으로 본문에 내용을 표시하였습니다.\n설명이 완료된 수식 중 참고할 사항은 margin컬럼에 표시였습니다.\n\n\n\n\nListing 1.2: Equation\n\n\n\\[\\frac{d}{dx}\\left( \\int_{a}^{x} f(u)\\,du\\right)=f(x).\\]\n\n\n\n\n\nWe know from the first fundamental theorem of calculus that for \\(x\\) in \\([a, b]\\):\n\\[\\frac{d}{dx}\\left( \\int_{a}^{x} f(u)\\,du\\right)=f(x).\\]\nCallout Example\n\n학습을 진행해 가는 과정에서 나오는 이슈사항은 Callout으로 표시해 두었습니다,\n각 Callout이 담아야 할 내용은 아래를 참고하여 주시기 바랍니다.\n\n\n\n\nListing 1.3: Callout\n\n\n\n\n\n\n\n\nNote의 활용법\n\n\n\n\n본문의 내용과 직접관련된 내용으로 부가적인 설명을 담고 있습니다.\n관련 문헌이나 자료들에서 중요한 부분을 발췌한 내용을 담고 있습니다.\n필자가 보다 효율적이라고 판단한 내용들을 보여주고자 합니다.\n\n\n\n\n\n\n\n\n\nTip의 활용법\n\n\n\n\n본문의 내용과 직접관련 없지만 알아두면 좋은 내용을 담고 있습니다.\n코드의 작성방법 등 유용한 정보를 답고 있습니다.\n실습과정에서 발견한 문제의 해결방법을 보여주고자 합니다.\n\n\n\n\n\n\n\n\n\nWarning의 활용법법\n\n\n\n\n이해하기 어려운 내용에 대하여 그 문제를 적시하고자 합니다.\n실습과정에서 경험한 문제 및 해결되지 않은 오류 등을 적시하고자 합니다.\n해결이 완료된 경우 note 또는 tip으로 전환될 수 있습니다.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "perceptron.html",
    "href": "perceptron.html",
    "title": "2  Perceptron",
    "section": "",
    "text": "2.1 What is a ‘Perceptron’?\n퍼셉트론은 인공신경망의 가장 기초가 되는 구조이고 이를 이해하기 위하여 생물한적 신경망의 가장 기초가 되는 뉴런에 대하여 살펴보고자(Figure 2.1) 합니다.\n뉴런의 구조는 다양하게 하지만 우리가 집중하고자 하는 곳은 크게 3가지 부분으로 구성되어 있습니다.\n여기서 주목할 것은 뉴런은 정보를 입력받아 내부적인 신호를 생성하여 다음 뉴런에 정보를 전달한 다는 것입니다.\n이러한 뉴런의 작동기전을 모방하여 만들어진 것이 퍼셉트론(Figure 2.2)입니다.\n퍼셉트론은 1957년 프랑크 로젠블라트가 제안한 것으로 퍼셉트론은 이러한 뉴런의 작동기전을 모방하여 정보를 입력(Input)받아 연산을 통해 출력(Output)을 생성하도록 설계되었습니다.\n이러한 정보의 흐름 또는 연산 절차를 다른 말로 Forward Propagation이라 합니다, 향후 논의 될 Back Propagation과 대비되는(?) 개념입니다.\nflowchart LR\n  subgraph node\n    direction LR\n    node2_1((SUM)) --&gt; node2_2((STEP))\n  end\n  node1_1((x_1)) --w1--&gt; node2_1\n  node1_2((x_2)) --w2--&gt; node2_1\n  node2_2 --&gt; node3((y_hat))\n\n\n\n\nFigure 2.2: Basic of Perceptron\nFigure 2.2 에서 원은 노드(Node)라고 하면 노드간 연결된 선을 엣지(Edge)라고 합니다. 엣지상에 존재하는 w는 가중치(Weight)하고 합니다.\n첫번째 노드의 x_1과 x_2는 입력값을 말하고, 두번째 노드는 내부적으로 SUM과 STEP으로 구성된 활성함수를 말하며, 마지막 노드의 z는 출력값으로 노드의 활성 정도를 말합니다.\n노란색 노드에서는 2단계 계산이 발생합니다. 하나늗 입력값의 뉴런에서는 신호의 세기를 계산하는 Weighted Sum(Equation 2.1) 과 뉴런의 여부를 계산하는 Step Function(Equation 2.2) 으로 구성되어 있습니다.\n결과적으로 퍼셉트론의 출력값(\\(\\hat{y}=h_{w,b}(x)=sign(\\textbf{w}^{T}\\textbf{x}+b)\\))은 (-1, 0, 1)로 3가지를 갖게 됩니다.\n\\[\nz = b + w_{1}x_{1} + w_{2}x_{2} + \\cdots + w_{n}x_{n} = b + \\textbf{w}^{T} \\textbf{x}\n\\tag{2.1}\\]\n\\[\nstep(z) = sign(z) = \\begin{cases}\n-1 & z &lt; 0 \\\\\n0 & z = 0 \\\\\n1 & z &gt; 0\n\\end{cases}\n\\tag{2.2}\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Perceptron</span>"
    ]
  },
  {
    "objectID": "perceptron.html#what-is-a-perceptron",
    "href": "perceptron.html#what-is-a-perceptron",
    "title": "2  Perceptron",
    "section": "",
    "text": "외부 자극등 정보를 수신하는 수상돌기(Dendrite)\n수신된 정보를 신호를 만들어 내는 핵(Nucleus)\n신호를 신경절달 물질로 만들어 내는 축삭돌기(Axon)+시넵스(Synapse)\n\n\n\n\n\n\n\n\nFigure 2.1: Structure of Neuron (source: Wikipdia)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n퍼셉트론에서 사용되는 계단함수\n\n\n\n일반적으로 Sign function을 가장 많이 사용하지만, 이진값(0, 1)만을 갖는 Heavisde step function이 사용되기도 합니다. Clsiffication 문제에서 직선상의 Observation값은 0 , Weight와 같은 방향은 1, 다른 방향은 -1로 처리하는 것이 보다 용이하기에 우리는 Sign function을 주로 사용합니다.\n\\[\nheavisde(z) = \\begin{cases}\n0 & z &lt; 0 \\\\\n1 & z &gt;= 0\n\\end{cases}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Perceptron</span>"
    ]
  },
  {
    "objectID": "perceptron.html#classification",
    "href": "perceptron.html#classification",
    "title": "2  Perceptron",
    "section": "2.2 Classification",
    "text": "2.2 Classification\n앞서 보았던 Perceptron을 Classification을 수행해보며 보다 자세하게 살펴보겠습니다. 이와 관련하여 우리는 Classification의 문제에 대하여 이해해볼 필요가 있습니다.\n\n2.2.1 What is a “Classicication”?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Perceptron</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "3  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Summary</span>"
    ]
  }
]