# Lab: Classification with Perceptron
---
퍼셉트론을 활용하여 **분류**(Classification) 문제를 풀어보도록 하겠습니다. 이를 해결해 가는 과정에서 다양한 정보를 학습할 수 있기를 기대하기 때문입니다.

먼저, 분류는 Supervised Learning으로 미리 리벨(Label)이 붙어 있는 데이터를 분류하는 문제입니다. 우리가 사용하는 방법은 선형적인 방법과 비선형적인 방법 등등을 살펴보겠습니다.

## Classification

우리가 실습할 분류 문제는 **선형 판별 분석**(Linear Discriminant Analysis)입니다. 수치형 입력변수를 받아 범주형 타겟 변수를 예측하는 분류 방법으로 퍼셈트론을 활용하여 해결하기 좋은 문제 입니다.

``` {python}
#| echo: false
import numpy as np
import matplotlib.pyplot as plt

# training data generation
m = 100
x1 = 8*np.random.rand(m, 1)
x2 = 7*np.random.rand(m, 1) - 4

g = 0.8*x1 + x2 -3

C1 = np.where(g >= 1)[0]
C0 = np.where(g < 1)[0]
```

```{python}
#| echo: false
#| label: fig-classification
#| fig-cap: "Example of LDA"
plt.figure(figsize=(6, 4))
plt.plot(x1[C1], x2[C1], 'ro', alpha = 0.4, label = "C1")
plt.plot(x1[C0], x2[C0], 'bo', alpha = 0.4, label = "C0")
plt.title('Liearly Seperable Classes', fontsize = 15)
plt.legend(loc = 1, fontsize = 15)
plt.xlabel(r'$x_1$', fontsize = 15)
plt.ylabel(r'$x_2$', fontsize = 15)
plt.show()
```

위 산점도(@fig-classification )와 같은 산점도에서 `C1`과 `C0`을 구분하는 문제를 퍼셉트론을 이용하여 해결해 보겠습니다.

전장에서 살펴본 퍼셉트론(@fig-perceptron1)에 따라 설명하고자 합니다. 우선 입력받는 변수는 $x$, 가중치는 $\omega$로 둘수 있습니다.

$$ 
\begin{align}
x = \begin{bmatrix}x_1\\ \vdots\\ x_d \end{bmatrix} \\
\omega = \begin{bmatrix}\omega_1\\ \vdots\\ \omega_d \end{bmatrix}
\end{align} 
$$ {#eq-perLab1}

여기서 Weighted Sum의 결과가 특정 임계치($\theta$)를 초과하는 지에 따라 적색 또는 청색으로 구분하고자 합니다.

$$ 
\begin{align}
C1 \quad if\, \sum_{i=1}^{d}\omega_{i}x_{i} > \theta \\
C0 \quad if\, \sum_{i=1}^{d}\omega_{i}x_{i} < \theta
\end{align} 
$$ {#eq-perLab2}


이를 다시 Step function을 활용하여 @fig-perceptron1의 노란색 박스($h(x)$)에 해당하는 노드를 다음의 식으로 변형하여 표현할 수 있습니다.

여기서 임계치($\theta$)는 편향에 해당하는 값으로 변형하고 이를 다시 입력변수 $\omega_0$으로 간단하게 표현할 수 있습니다.

$$ 
\begin{align}
h(x) &= \text{sign} \left(\left( \sum\limits_{i=1}^{d}\omega_ix_i \right)- \theta \right) \\
&= \text{sign}\left(\left( \sum\limits_{i=1}^{d}\omega_ix_i \right)+ \omega_0\right)
\end{align} 
$$ {#eq-perLab3}

$\omega_0=1$로 두고 이를 벡터 형식(vector form)으로 표현하고, Step function을 Sign function으로 정의하면 최종식은 아래와 같이 수정할 수 있습니다.

$$ 
\begin{align}
h(x) &= \text{sign}(\sum_{i=0}^{d}\omega_{i}x_{i}) \\
&=\text{sign}(\omega^{T}x) \\
\text{sign}(x) &= \begin{cases}
1, &\text{if }\; x > 0\\
0, &\text{if }\; x = 0\\
-1, &\text{if }\; x < 0
\end{cases}
\end{align} 
$$ {#eq-perLab4}