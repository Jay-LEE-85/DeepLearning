{
  "hash": "33dc6bc8ab6d8c8acc6114b560a7205c",
  "result": {
    "engine": "jupyter",
    "markdown": "# Forward Propagation\n\n---\n\n\n\n신경망에서의 데이터의 기본적인 입력층에서 출력까지의 데이터의 흐름(처리 과정)에 대하여 살펴보겠습니다.\n\n아래에 예시는 입력층(0층)의 노드 2개, 은닉층(1층)의 노드 3개, 은닉층(2층)의 노드 2개를 거쳐 2개의 노드를 갖는 출력층(3층)으로 구성된 신경망 입니다.\n\n![signaling](/image/ann.png){#fig-forANN0 width=70%}\n\n@fig-forANN0 에서 $a^{(1)}_1$의 경우 입력갑의 경우 가중치와 곱한 값을 합산한 Weighted Sum 값과 편향(bias)더하여 아래와 같이 산출할 수 있고 이를 Vector form으로 간소화 할 수 있습니다.\n\n$$ \n\\begin{align}\na^{(1)}_{1} &= w^{(1)}_{11}+x^{}_{1}+w^{(1)}_{12}x_2+b^{(1)}_{1} \\\\\n\\textbf{A}^{(1)} &= \\textbf{X}\\textbf{W}^{(1)} + \\textbf{B}^{(1)}\n\\end{align}\n$$ {#eq-forANN}\n\n::: {.column-margin}\n$$\n\\begin{align}\n\\textbf{A}^{(1)} &= (a^{(1)}_{1}\\, a^{(1)}_{2}\\, a^{(1)}_{3}) \\\\\n\\textbf{X} &= (x_1\\, x_2) \\\\\n\\textbf{B}^{(1)} &= (b^{(1)}_{1}, b^{(1)}_{2}, b^{(1)}_{3}) \\\\\n\\textbf{W}^{(1)} &= \\begin{pmatrix}\n    w^{(1)}_{11} & w^{(1)}_{21} & w^{(1)}_{31} \\\\\n    w^{(1)}_{12} & w^{(1)}_{22} & w^{(1)}_{32} \\\\\n\\end{pmatrix}\n\\end{align}\n$$\n:::\n\n위의 식(@eq-forANN )에 따라 각 층(layer)의 신호 전달과정을 구현해 보도록 하겠습니다.\n\n\n## Layer-by-Layer signaling\n\n3층 신경망의 신호 전달과정은 아래와 같이 Weighted Sum에 기반하며 그 값을 다시 화성화 함수(가령 Sigmoid)를 통하여 노드의 값이 최종 산출됩니다.\n\n또한, 이전층의 값들을 받아 산출된 값은 다시 입력값으로 하여 다음층으로 전달되는 과정을 거쳐 최종적으로 출력층까지 이 과정을 반복하게 됩니다. 이 과정이 신호 전달 또는 Forward propagation 입니다.\n\n::: {#fig-forANN layout-ncol=3}\n![input-hidden](/image/ann1.png){#fig-forANN1}\n\n![hidden-hidden](/image/ann2.png){#fig-forANN2}\n\n![hidden-output](/image/ann3.png){#fig-forANN3}\n\nProcess of Forward Propagation\n:::\n\n**input to hidden**\n\n최초 입력층(0층)의 신호 전달 체계는 입력값(노드)은 2개인 1차원 배열이고 다음의 은닉층(1층)은 노드 3계로 이루어진 1차원 배열입니다. 2개의 노드 값을 받아 3개의 노드로 전달해야 하므로 노드간의 간선은 총 6개($6 = 2 \\times 3$)입니다.\n\n입력값 및 편향값을 `a`에 전달하고 `a`값을 활성화 함수(Sigmoid 함수를 사용) `h()`를 이용하여 신호 `z`를 산출하도록 합니다. 결과적으로 은닉층(1층)에 해당하는 3개의 노드의 신호를 확인할 수 있습닏.\n\n::: {#71ad11b3 .cell execution_count=2}\n``` {.python .cell-code}\n# 입력값, 편향, 가중치\nX  = np.array([1.0, 0.5])\nB1 = np.array([0.1,0.2,0.3])\nW1 = np.array([[0.1,0.3,0.5], [0.2,0.4,0.6]])\n\n# Weighted Sum\nA1 = np.dot(X, W1) + B1\n\n# Activation Function\nZ1 = sigmoid(A1)\n\nprint(A1)\nprint(Z1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[0.3 0.7 1.1]\n[0.57444252 0.66818777 0.75026011]\n```\n:::\n:::\n\n\n**hidden to hidden**\n\n은닉층(1층)이 다시 입력층으로 하여 다음의 은닉층(2층)으로 신호를 전달하도록 해야 합니다. 앞서 진행한 신호 전달 과정과 동일합니다.\n\n다만, 입력 노드가 편향을 포함하여 4개가 다음 층인 2개의 노드로 전달됨에 따라 이전 과정과 달리 간선은 총 8개 입니다. 편향은 2개 간선을 갖고 가중치는 입력 노드별 2개 총 6개로 이루어 집니다.\n\n::: {#fe4126cb .cell execution_count=3}\n``` {.python .cell-code}\n# 편향, 가중치\nB2 = np.array([0.1,0.2])\nW2 = np.array([[0.1,0.4], [0.2,0.5], [0.3,0.6]])\n\n# Weighted Sum & Activation Function\nA2 = np.dot(Z1, W2) + B2\nZ2 = sigmoid(A2)\n\nprint(A2)\nprint(Z2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[0.51615984 1.21402696]\n[0.62624937 0.7710107 ]\n```\n:::\n:::\n\n\n**hidden to output**\n\n은닉층(2층)이 다시 입력층으로 하여 다음의 출력층(3층)으로 신호를 전달하도록 해야 합니다. 앞서 진행한 신호 전달 과정과 동일합니다.\n\n주의할 것은 노드별 \n\n::: {#caba4da3 .cell execution_count=4}\n``` {.python .cell-code}\n# 편향, 가중치\nB2 = np.array([0.1,0.2])\nW2 = np.array([[0.1,0.4], [0.2,0.5], [0.3,0.6]])\n\n# Weighted Sum & Activation Function\nA2 = np.dot(Z1, W2) + B2\nZ2 = sigmoid(A2)\n\nprint(A2)\nprint(Z2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[0.51615984 1.21402696]\n[0.62624937 0.7710107 ]\n```\n:::\n:::\n\n\n",
    "supporting": [
      "ANN-forward_files"
    ],
    "filters": [],
    "includes": {}
  }
}