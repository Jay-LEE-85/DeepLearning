{
  "hash": "f0dd8a098710e63b6fbcd1786c53ea54",
  "result": {
    "engine": "jupyter",
    "markdown": "# Forward Propagation {#sec-annFor}\n\n---\n\n\n\n신경망에서의 데이터의 기본적인 입력층에서 출력까지의 데이터의 흐름(처리 과정)에 대하여 살펴보겠습니다.\n\n아래에 예시는 입력층(0층)의 노드 2개, 은닉층(1층)의 노드 3개, 은닉층(2층)의 노드 2개를 거쳐 2개의 노드를 갖는 출력층(3층)으로 구성된 신경망 입니다.\n\n![signaling(souce: Deeplearning from Scratch)](/image/ann.png){#fig-forANN0 width=70%}\n\n@fig-forANN0 에서 $a^{(1)}_1$의 경우 입력갑의 경우 가중치와 곱한 값을 합산한 Weighted Sum 값과 편향(bias)더하여 아래와 같이 산출할 수 있고 이를 Vector form으로 간소화 할 수 있습니다.\n\n$$ \n\\begin{align}\na^{(1)}_{1} &= w^{(1)}_{11}+x^{}_{1}+w^{(1)}_{12}x_2+b^{(1)}_{1} \\\\\n\\textbf{A}^{(1)} &= \\textbf{X}\\textbf{W}^{(1)} + \\textbf{B}^{(1)}\n\\end{align}\n$$ {#eq-forANN}\n\n::: {.column-margin}\n$$\n\\begin{align}\n\\textbf{A}^{(1)} &= (a^{(1)}_{1}\\, a^{(1)}_{2}\\, a^{(1)}_{3}) \\\\\n\\textbf{X} &= (x_1\\, x_2) \\\\\n\\textbf{B}^{(1)} &= (b^{(1)}_{1}, b^{(1)}_{2}, b^{(1)}_{3}) \\\\\n\\textbf{W}^{(1)} &= \\begin{pmatrix}\n    w^{(1)}_{11} & w^{(1)}_{21} & w^{(1)}_{31} \\\\\n    w^{(1)}_{12} & w^{(1)}_{22} & w^{(1)}_{32} \\\\\n\\end{pmatrix}\n\\end{align}\n$$\n:::\n\n위의 식(@eq-forANN )에 따라 각 층(layer)의 신호 전달과정을 구현해 보도록 하겠습니다.\n\n\n## Layer-by-Layer signaling\n\n3층 신경망의 신호 전달과정은 아래와 같이 Weighted Sum에 기반하며 그 값을 다시 화성화 함수(가령 Sigmoid)를 통하여 노드의 값이 최종 산출됩니다.\n\n또한, 이전층의 값들을 받아 산출된 값은 다시 입력값으로 하여 다음층으로 전달되는 과정을 거쳐 최종적으로 출력층까지 이 과정을 반복하게 됩니다. 이 과정이 신호 전달 또는 Forward propagation 입니다.\n\n::: {#fig-forANN layout-ncol=3}\n![input-hidden](/image/ann1.png){#fig-forANN1}\n\n![hidden-hidden](/image/ann2.png){#fig-forANN2}\n\n![hidden-output](/image/ann3.png){#fig-forANN3}\n\nProcess of Forward Propagation(souce: Deeplearning from Scratch)\n:::\n\n**input to hidden**\n\n최초 입력층(0층)의 신호 전달 체계는 입력값(노드)은 2개인 1차원 배열이고 다음의 은닉층(1층)은 노드 3계로 이루어진 1차원 배열입니다. 2개의 노드 값을 받아 3개의 노드로 전달해야 하므로 노드간의 간선은 총 6개($6 = 2 \\times 3$)입니다.\n\n입력값 및 편향값을 `a`에 전달하고 `a`값을 활성화 함수(Sigmoid 함수를 사용) `h()`를 이용하여 신호 `z`를 산출하도록 합니다. 결과적으로 은닉층(1층)에 해당하는 3개의 노드의 신호를 확인할 수 있습닏.\n\n::: {#1787c097 .cell execution_count=2}\n``` {.python .cell-code}\n# 입력값, 편향, 가중치\nX  = np.array([1.0, 0.5])\nB1 = np.array([0.1,0.2,0.3])\nW1 = np.array([[0.1,0.3,0.5], [0.2,0.4,0.6]])\n\n# Weighted Sum\nA1 = np.dot(X, W1) + B1\n\n# Activation Function\nZ1 = sigmoid(A1)\n\nprint(A1)\nprint(Z1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[0.3 0.7 1.1]\n[0.57444252 0.66818777 0.75026011]\n```\n:::\n:::\n\n\n**hidden to hidden**\n\n은닉층(1층)이 다시 입력층으로 하여 다음의 은닉층(2층)으로 신호를 전달하도록 해야 합니다. 앞서 진행한 신호 전달 과정과 동일합니다.\n\n다만, 입력 노드가 편향을 포함하여 4개가 다음 층인 2개의 노드로 전달됨에 따라 이전 과정과 달리 간선은 총 8개 입니다. 편향은 2개 간선을 갖고 가중치는 입력 노드별 2개 총 6개로 이루어 집니다.\n\n::: {#d49b9e09 .cell execution_count=3}\n``` {.python .cell-code}\n# 편향, 가중치\nB2 = np.array([0.1,0.2])\nW2 = np.array([[0.1,0.4], [0.2,0.5], [0.3,0.6]])\n\n# Weighted Sum & Activation Function\nA2 = np.dot(Z1, W2) + B2\nZ2 = sigmoid(A2)\n\nprint(A2)\nprint(Z2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[0.51615984 1.21402696]\n[0.62624937 0.7710107 ]\n```\n:::\n:::\n\n\n**hidden to output**\n\n은닉층(2층)이 다시 입력층으로 하여 다음의 출력층(3층)으로 신호를 전달하도록 해야 합니다. 앞서 진행한 신호 전달 과정과 동일합니다.\n\n주의할 것은 출력층의 경우 해결하고자 하는 문제의 성질에 맞게 설정되어야 합니다. 여기서는 입력되는 값을 그대로 출력하는 항등함수(Identity Function)알 사용하도록 하겠습니다.\n\n::: {#aa1eba1d .cell execution_count=4}\n``` {.python .cell-code}\n# 항등함수\ndef identity_function(x):\n  return x\n\n# 편향, 가중치\nB3 = np.array([0.1,0.2])\nW3 = np.array([[0.1,0.3], [0.2,0.4]])\n\n# Weighted Sum & Activation Function\nA3 = np.dot(Z2, W3) + B3\nY  = identity_function(A3) # Y = A3\n\nprint(Y)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[0.31682708 0.69627909]\n```\n:::\n:::\n\n\n::: {.callout-note}\n## 출력층의 활성화 함수\n-   출력층의 활성화 함수는 문제의 성질에 맞춰야 한다고 하였습니다.\n-   문제의 성질은 크게 2가지로 나누어 볼 수 있는데 하나는 **분류**(classfication), 다른 하나나는 **회귀**(regression)입니다.\n-   각각의 문제에 맞는 활성화 함수는 다양하며 자세한 사항은 지속적으로 @sec-actANN 에 내용을 추가하도록 하겠습니다.\n:::\n\n**Wrap-up**\n\n앞서 정리한 내용을 하나의 모듈로 작성하도록 하겠습니다. 이 신경망의 신호 전달 과정은 순방향의 연산 과정만을 익히기 위함이고 가장 처음에 실행되는 과정입니다.\n\n::: {#853b45bd .cell execution_count=5}\n``` {.python .cell-code}\ndef init_network(): # <1>\n  network = {}\n  network['W1'] = np.array([[0.1,0.3,0.5], [0.2,0.4,0.6]]) # 입력2 출력3\n  network['b1'] = np.array([0.1,0.2,0.3])\n  network['W2'] = np.array([[0.1,0.4], [0.2,0.5], [0.3,0.6]]) # 입력3, 출력2\n  network['b2'] = np.array([0.1,0.2])\n  network['W3'] = np.array([[0.1,0.3], [0.2,0.4]]) # 입력2, 출력2\n  network['b3'] = np.array([0.1,0.2])\n\n  return network\n\ndef forward(network, x): # <2>\n  W1, W2, W3 = network['W1'], network['W2'], network['W3']\n  b1, b2, b3 = network['b1'], network['b2'], network['b3']\n\n  a1 = np.dot(x, W1) + b1\n  z1 = sigmoid(a1)\n  a2 = np.dot(z1, W2) + b2\n  z2 = sigmoid(a2)\n  a3 = np.dot(z2, W3) + b3\n  y  = identity_function(a3)\n\n  return Y\n\nnetwork = init_network()\nx = np.array([1.0,5.0])\ny = forward(network, x)\n\nprint(y)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[0.31682708 0.69627909]\n```\n:::\n:::\n\n\n1. 가중치와 편향을 초기화하고 이들을 닉셔너리 변수인 network에 저장\n2. 입력신호를 출력으로 변환하는 처리과정\n\n## Disign Output layer\n\n신경망의 경우 통상 회귀의 경우 항등 함수를, 분류의 경우는 소프트맥스 함수(softmax function)를 사용합니다. 소프트맥스 함수의 식은 아래와 같습니다.\n\n$$\ny_k = \\frac{exp(a_k)}{\\sum^{n}_{i=1}exp(a_i)}\n$$ {#eq-softmax}\n\n위 식에서 $exp(x)$는 $e^x$를 지수함수를 의미하며, $n$은 출력층의 뉴런수, $y_k$는 출력노드 중 $k$번째를 의미합니다. 분자는 $k$번째 출력노드의 값을 분모는 전체 출력노드의 합을 의미합니다.\n\n``` {mermaid}\nflowchart LR\n  subgraph h [\"sigma()\"]\n    direction LR\n    a1((a1)) & a2((a2)) & a3((a3))\n    y1((y1)) & y2((y2)) & y3((y3))\n  end\n\n  a1 & a2 & a3 ---> y1 & y2 & y3\n```\n\n**Cautions for implementing the Softmax function**\n\n@eq-softmax 식을 코드로 구현하기 이전에 주의할 사항이 필요합니다. \n\n하나는 오퍼플로(overflow), 즉 컴퓨터의 특성상 너무 큰 수의 경우 `Inf`가 나오게 된다는 점입니다.\n\n이러한 문제를 해결하기 위하여 참고한 자료에는 임의 상수 $C$를 분모와 분자에 모두 곱해주는 방식으로 이 문제를 해결 할수 있다고 하며 $C$는 다시 exp의 지수항으로 옮기고 $C'$로 변경할 수 있습니다.\n\n$$\n\\begin{align}\ny_k = \\frac{exp(a_k)}{\\sum^{n}_{i=1}exp(a_i)} &= \\frac{C\\,exp(a_k)}{C\\,\\sum^{n}_{i=1}exp(a_i)} \\\\\n&= \\frac{exp(a_k+log C)}{\\sum^{n}_{i=1}exp(a_i+log C)} \\\\\n&= \\frac{exp(a_k+C')}{\\sum^{n}_{i=1}exp(a_i+C')}\n\\end{align}\n$$ {#eq-softmax2}\n\n위의 식에 따라 출력층에 사용할 소프트맥스 함수를 아래와 같이 구현할 수 있으며, 개선된 식의 $C$는 통상 입력값의 최대값으로 설정하도록 하겠습니다.\n\n::: {#6d5c0c7a .cell execution_count=6}\n``` {.python .cell-code}\ndef softmax(a):\n  c = np.max(a)\n  exp_a = np.exp(a-c)\n  sum_exp_a = np.sum(exp_a)\n  y = exp_a / sum_exp_a\n\n  return y\n\na = np.array([0.3, 2.9, 4.0])\ny = softmax(a)\n\nprint(y)\n\nnp.sum(y)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[0.01821127 0.24519181 0.73659691]\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=6}\n```\n1.0\n```\n:::\n:::\n\n\n위이 함수를 실행하면 출력값의 총합은 1임을 알 수 있습니다. 이것은 출력된 개별 값들을 확률로 해석할 수 있음을 의미합니다. 다만, 지수합수인 `exp()`계산시 자원이 많이 소비됨에 따라 추론 단계에서는 소프트맥스 함수를 생랙하기도 한다고 합니다.\n\n## Lab : Number recognition with MNIST\n\n현재 우리가 진행하고 있는 과정은 학습과 추론 중 **추론**(inference)에 해당하는 **순전파**(forward propagation)입니다. 학습의 경우 **역전파**(back propagation)를 통하여 가중치를 업데이트 하나 추론의 경우는 설정된 가중치를 이용하여 문제를 해결하는 과정입니다.\n\n::: {.callout-note}\n## MNIST 데이터셋\nMNIST^[ANN 및 CNN 까지 다양한 예제에 활용될 예정] 데이터셋은 기계 학습 분야에서 널리 사용되는 손으로 쓴 숫자 이미지 데이터셋입니다. 이 데이터셋은 0부터 9까지의 숫자를 손으로 쓴 28x28 픽셀 크기의 이미지로 구성되어 있습니다. 주로 숫자 인식 및 분류 알고리즘의 테스트 및 훈련에 사용됩니다.\n\n-   **크기**: 28x28 픽셀\n-   **포맷**: 흑백 이미지(1채널)\n-   **이미지 개수**:\n        - 훈련 데이터: 60,000개\n        - 테스트 데이터: 10,000개\n-   **픽셀 값 범위**: 0부터 255까지\n:::\n\n::: {#87d18d08 .cell execution_count=7}\n``` {.python .cell-code}\nimport sys, os\nsys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\nimport numpy as np\nfrom dataset.mnist import load_mnist\nfrom PIL import Image\n\n# MNIST 데이터셋 로드\n(x_train, t_train), (x_test, t_test) = \\\n  load_mnist(\n    flatten = True,        # <1>\n    normalize = False,     # <2>\n    one_hot_label = False  # <3> \n    ) \n\ndef img_show(img):\n    pil_img = Image.fromarray(np.uint8(img))\n    pil_img.show()\n\nimg = x_train[0]\nlabel = t_train[0]\nprint(label)  # 5\n\nprint(img.shape)  # (784,)\nimg = img.reshape(28, 28)  # 형상을 원래 이미지의 크기로 변형\nprint(img.shape)  # (28, 28)\n\nimg_show(img)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n5\n(784,)\n(28, 28)\n```\n:::\n:::\n\n\n1. `flatten`은 28x28의 2D-배열을 784x1 1D배열로 만들지 말지 결정하는 변수\n2. `normalize`는 픽셀값의 범위를 기존 [0, 255]에서 [0.0, 1.0]으로 변환할지 말지를 결정하는 변수\n3. `one_hot_label`은 레이블의 값을 정수(False, 예:5)로 할지, 한 원소만을 1로 갖는 배열(True, 예:[0,0,0,0,0,1,0,0,0,0,0])로 할지 결정하는 변수\n\n### Inference processing\n\nANN을 활용하여 MNIST 데이터셋을 가지고 추론과정을 신경망으로 구현하면 아래와 같습니다. 입력층의 뉴런은 $28\\times28$의 데이터를 받아 Flatten하게 $784$개의 뉴런으로 갖도록 합니다. 그리고 출력층의 뉴런은 0~9까지 10개로 분류해야 하므로 10개의 뉴런을 갖도록 합니다.\n\n입력과 출력사이의 은닉층은 2개의 층으로 구성하도록 하고 각각 50개 100개의 뉴런을 갖도록 합니다. 은닉층의 뉴런의 갯수는 임의로 정한 것이고 본 사전에 학습된 $\\omega$ 를 사용하여 추론의 정확도를 평가해 보도록 하겠습니다.\n\n::: {#785a06e7 .cell execution_count=8}\n``` {.python .cell-code}\n# coding: utf-8\nimport sys, os\nsys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\nimport numpy as np\nimport pickle\nfrom dataset.mnist import load_mnist\nfrom common.functions import sigmoid, softmax\n\n\ndef get_data():\n    (x_train, t_train), (x_test, t_test) = load_mnist(\n      normalize=True, # <4>\n      flatten=True, \n      one_hot_label=False)\n    return x_test, t_test\n\n\ndef init_network():\n    with open(\"dataset/sample_weight.pkl\", 'rb') as f:\n        network = pickle.load(f)\n    return network\n\n\ndef predict(network, x):\n    W1, W2, W3 = network['W1'], network['W2'], network['W3']\n    b1, b2, b3 = network['b1'], network['b2'], network['b3']\n\n    a1 = np.dot(x, W1) + b1\n    z1 = sigmoid(a1)\n    a2 = np.dot(z1, W2) + b2\n    z2 = sigmoid(a2)\n    a3 = np.dot(z2, W3) + b3\n    y = softmax(a3)\n\n    return y\n\n\nx, t = get_data()\nnetwork = init_network()\naccuracy_cnt = 0\nfor i in range(len(x)): # <1>\n    y = predict(network, x[i]) # <1>\n    p= np.argmax(y) # <2>\n    if p == t[i]: # <3>\n        accuracy_cnt += 1 # <3>\n\nprint(\"Accuracy:\" + str(float(accuracy_cnt) / len(x)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAccuracy:0.9352\n```\n:::\n:::\n\n\n1. `for`문 안에서 이미지 1장씩 꺼내어 `predict()`함수로 분류(0~9)를 실행하여 레이블의 확률을 Numpy 배열로 반환\n2. `np.argmax`로 반환된 레이블 배열에서 가장 높은 값(확률)의 인덱스를 산출\n3. 정답 레이블과 산출 레이블의 비교하여 일치하면 `accuracy_cnt`로 정답 갯수 업데이트\n4. `load_mnist`함수의 인자 중 `normalize`가 `True`는 데이터를 0~1사이의 값으로 정규화 한다는 의미\n\n위의 과정을 통해 분류의 정확도는 93.52%임을 확인 할 수 있습니다. 이후에 이 정확도를 향상시키기 위한 신경망의 학습 등에 대하여 살펴볼 예정입니다.\n\n",
    "supporting": [
      "ANN-forward_files"
    ],
    "filters": [],
    "includes": {}
  }
}