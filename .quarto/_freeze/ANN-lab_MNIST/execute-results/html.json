{
  "hash": "57457ef08b58a86bd6701b83967da663",
  "result": {
    "engine": "jupyter",
    "markdown": "# Lab: Implementing ANN with MINIST dataset\n\n\nANN에서 필요한 활성화함수, 손실함수, 미니배치 및 경사하강법(기울기)에 학습내용을 바탕으로 신경망을 실제로 구현해보도록 하겠습니다. 참고로 미니배치를 이용하여 확률적으로 데이터를 무작위로 추출한 경우를 **확률적 경사하강법**(SGD: stochastic gradient descent)라고 합니다.\n\n구현할 학습알고리즘은 MNIST 데이터 셋을 활용하여 손글씨 숫자를 인식하는 ANN으로 먼저 ANN 클래스를 구현하고 이후 미니배치를 통한 학습 알고리즘을 구현하도록 하겠습니다.^[TwoLayerNet은 스탠퍼드의 CS231n  수업에서 제공한 코드를 참고]\n\n### STEP1: Tow Layer Net\n\n::: {#f8c6d664 .cell execution_count=1}\n``` {.python .cell-code}\nimport sys, os\nsys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\nfrom common.functions import *\nfrom common.gradient import numerical_gradient\n\n\nclass TwoLayerNet:\n\n    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n        # 가중치 초기화\n        self.params = {}\n        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n        self.params['b1'] = np.zeros(hidden_size)\n        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n        self.params['b2'] = np.zeros(output_size)\n\n    def predict(self, x):\n        W1, W2 = self.params['W1'], self.params['W2']\n        b1, b2 = self.params['b1'], self.params['b2']\n    \n        a1 = np.dot(x, W1) + b1\n        z1 = sigmoid(a1)\n        a2 = np.dot(z1, W2) + b2\n        y = softmax(a2)\n        \n        return y\n        \n    # x : 입력 데이터, t : 정답 레이블\n    def loss(self, x, t):\n        y = self.predict(x)\n        \n        return cross_entropy_error(y, t)\n    \n    def accuracy(self, x, t):\n        y = self.predict(x)\n        y = np.argmax(y, axis=1)\n        t = np.argmax(t, axis=1)\n        \n        accuracy = np.sum(y == t) / float(x.shape[0])\n        return accuracy\n        \n    # x : 입력 데이터, t : 정답 레이블\n    def numerical_gradient(self, x, t):\n        loss_W = lambda W: self.loss(x, t)\n        \n        grads = {}\n        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n        \n        return grads\n        \n    def gradient(self, x, t):\n        W1, W2 = self.params['W1'], self.params['W2']\n        b1, b2 = self.params['b1'], self.params['b2']\n        grads = {}\n        \n        batch_num = x.shape[0]\n        \n        # forward\n        a1 = np.dot(x, W1) + b1\n        z1 = sigmoid(a1)\n        a2 = np.dot(z1, W2) + b2\n        y = softmax(a2)\n        \n        # backward\n        dy = (y - t) / batch_num\n        grads['W2'] = np.dot(z1.T, dy)\n        grads['b2'] = np.sum(dy, axis=0)\n        \n        da1 = np.dot(dy, W2.T)\n        dz1 = sigmoid_grad(a1) * da1\n        grads['W1'] = np.dot(x.T, dz1)\n        grads['b1'] = np.sum(dz1, axis=0)\n\n        return grads\n```\n:::\n\n\n### STEP2: Mini-Batch\n\n\n\n### STEP3: Evaluating with test data\n\n::: {#a6b5bbf1 .cell execution_count=3}\n``` {.python .cell-code}\nimport sys, os\nsys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom dataset.mnist import load_mnist\n\n# 데이터 읽기\n(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n\nnetwork = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n\n# 하이퍼파라미터\niters_num = 10000  # 반복 횟수를 적절히 설정한다.\ntrain_size = x_train.shape[0]\nbatch_size = 100   # 미니배치 크기\nlearning_rate = 0.1\n\ntrain_loss_list = []\ntrain_acc_list = [] # <1>\ntest_acc_list = [] # <1>\n\n# 1에폭당 반복 수\niter_per_epoch = max(train_size / batch_size, 1) # <1>\n\nfor i in range(iters_num):\n    # 미니배치 획득\n    batch_mask = np.random.choice(train_size, batch_size)\n    x_batch = x_train[batch_mask]\n    t_batch = t_train[batch_mask]\n    \n    # 기울기 계산\n    #grad = network.numerical_gradient(x_batch, t_batch)\n    grad = network.gradient(x_batch, t_batch)\n    \n    # 매개변수 갱신\n    for key in ('W1', 'b1', 'W2', 'b2'):\n        network.params[key] -= learning_rate * grad[key]\n    \n    # 학습 경과 기록\n    loss = network.loss(x_batch, t_batch)\n    train_loss_list.append(loss)\n    \n    # 1에폭당 정확도 계산\n    if i % iter_per_epoch == 0: # <2>\n        train_acc = network.accuracy(x_train, t_train)\n        test_acc = network.accuracy(x_test, t_test)\n        train_acc_list.append(train_acc)\n        test_acc_list.append(test_acc)\n        print(\"train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))\n\n# 그래프 그리기\nmarkers = {'train': 'o', 'test': 's'}\nx = np.arange(len(train_acc_list))\nplt.plot(x, train_acc_list, label='train acc')\nplt.plot(x, test_acc_list, label='test acc', linestyle='--')\nplt.xlabel(\"epochs\")\nplt.ylabel(\"accuracy\")\nplt.ylim(0, 1.0)\nplt.legend(loc='lower right')\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntrain acc, test acc | 0.09643333333333333, 0.0956\ntrain acc, test acc | 0.7747833333333334, 0.782\ntrain acc, test acc | 0.8786833333333334, 0.8801\ntrain acc, test acc | 0.89785, 0.9012\ntrain acc, test acc | 0.9082166666666667, 0.9117\ntrain acc, test acc | 0.9159666666666667, 0.9186\ntrain acc, test acc | 0.9206166666666666, 0.9231\ntrain acc, test acc | 0.9252, 0.9268\ntrain acc, test acc | 0.9294, 0.9313\ntrain acc, test acc | 0.9321166666666667, 0.9333\ntrain acc, test acc | 0.9361333333333334, 0.9353\ntrain acc, test acc | 0.93825, 0.9381\ntrain acc, test acc | 0.9413333333333334, 0.9399\ntrain acc, test acc | 0.9434, 0.942\ntrain acc, test acc | 0.9449333333333333, 0.9437\ntrain acc, test acc | 0.94715, 0.9456\ntrain acc, test acc | 0.9484833333333333, 0.9464\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](ANN-lab_MNIST_files/figure-html/cell-4-output-2.png){width=589 height=434}\n:::\n:::\n\n\n",
    "supporting": [
      "ANN-lab_MNIST_files"
    ],
    "filters": [],
    "includes": {}
  }
}