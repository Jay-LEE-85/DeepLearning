{
  "hash": "f8b439c42fae3f1741cacd28210b1bd9",
  "result": {
    "engine": "jupyter",
    "markdown": "# Learning process\n\n---\n\n\n\n인공싱경망은 학습과 추론 과정으로 구분할 수 있습니다. 인공신경망을 만든다(?)는 관점은 학습을 의미하며 이는 신경망의 출력값과 실제값의 차이를 줄이는 방향으로 신경망을 이루는 가중치(매개변수)를 찾는 과정이라고 단순히 말할 수 있습니다.\n\n여기서 중요한 것은 인간이 매개변수를 설정하지 않고 기계가 스스로 찾는 다는 점에서 매우 의미가 있습니다. \n\n퍼셉트론에서 진리표의 매개변수는 3개에 불과하여 수작업이 가능했지만 매개변수가 1000개라고 하면 수작업이 불가할 것 입니다. 이 과정을 기계가 스스로 학습하여 해결한다니 매우 놀라운 일입니다.\n\n그리고 학습을 통해 매개변수를 기계가 찾는 다는 것은 주어진 데이터의 특성(Features)를 추출하여 특정 패턴을 찾는 것입니다. 물론 사람의 개입없이 말입니다.\n\n우리가 집중할 것은 이러환 과정이 어떻게 구현되는가 입니다. 그래서 다음의 주제에 대하여 집중하여 살펴보도록 하겠습니다.\n\n1.  출력값과 실제값을 비교하여 정확도를 측정하는 **손실함수**(loss function)\n2.  손실값을 활용한 가중치 갱신을 위한 **경사하강법**(gradient descent)\n    -   pre-requisite: 수치미분을 통한 기울기 산출\n    -   applicatoin: 학습률(learing rate $\\eta$: $\\omega = \\omega - \\eta\\frac{\\partial{f}}{\\partial{\\omega}}$)을 이용한 매개변수 갱신\n\n``` {mermaid}\n%%| label: fig-annLearn\n%%| fig-cap: \"Forward and Backward propagation\"\n\nflowchart LR\n  input[input] --\"forward\" ---> loss[loss]\n  loss[loss] --\"backward\" ---> input[input]\n```\n\n손실함수는 추론(?)에 해당하는 순전파 과정에서 그 정확도를 산출하기 위한 내용이고, 경사하강법은 산출된 Loss를 입력까지의 역전파하여 매개변수인 가중치를 업데이트 하는 과정에 대한 내용으로 단순하게 이해할 수 있고 자세한 사항을 아래에 섦영하도로고 하겠습니다.\n\n## Loss Function\n\n손실함수는 ANN에 입력되어 산출되는 값과 실제 값을 비교하여 **정확도**를 측정하는 하나의 **지표**라고 할 수 있다. ANN은 이러한 지표를 기준으로 최적의 가중치를 탐색하여 하나의 모델을 구성하게 됩니다.\n\n손실함수로는 일반적으로 **오차제곱합**(SSE, sum of squares for error)과 **교차 엔트로피 오차**(CEE, cross entropy error)를 사용합니다.\n\n### SSE(sum of squares for error)\n\n오차제곱합(@eq-annSSE )은 신경말의 출력값($y_k$)과 실제값($t_k$) 사이의 차이인 오차를 계산하고 이 오차를 제곱하여 모두 더한 값을 말하며 이 값이 작아질 수록 모델이 더 좋은 예측능력을 보유한다고 판단합니다.\n\n$$\nE = \\frac{1}{2}\\sum^{}_{k}(y_k - t_k)^2\n$$ {#eq-annSSE}\n\n오차제곱합의 작동원리를 이해하기 위하여 임의로 **원-핫 인코딩**된 레이블($t_k$)과 임의로 **소프트맥수 합수**의 출력값($y_k$)을 생성하여 코드로 구현해 보겠습니다.^[CEE에서도 동일한 예제를 사용 예정]\n\n::: {#9917ec1e .cell execution_count=2}\n``` {.python .cell-code}\n# 정답 레이블은 2\nt = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n\ndef sum_squres_error(y, t):\n  return 0.5 * np.sum((y-t)**2)\n\n# 예1: 2일 확률이 제일 높음(60%)\ny = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\nl1 = sum_squres_error(np.array(y), np.array(t))\n\n# 예2: 7일 확률이 제일 높음(60%)\ny = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]\nl2 = sum_squres_error(np.array(y), np.array(t))\n\nprint(\"1번예제: %3f, 2번예제:%3f\" % (l1, l2))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1번예제: 0.097500, 2번예제:0.597500\n```\n:::\n:::\n\n\n정답 레이블인 `2`를 높은 확률로 산출한 1번 예제의 손실값이 0.09로 0에 근접하고, `7`를 높은 확률로 산출한 2번 예제의 손실값이 0.59로 0에 멀게 산출됨을 확인 할 수 있습니다.\n\n### CEE(cross entropy error)\n\n교차 엔트로피(@eq-annCEE )는 신경망의 출력값($y_k$)이 소프트 맥스 함수를 거쳐 확률로 [0.0, 1.0]의 값을 갖는 다는 점을 고려하여 확률값이 높을 수록 0에 수렴하는 손실값을 산출합니다.\n\n$$\nE = -\\sum^{}_{k}t_k\\log{y_k}\n$$ {#eq-annCEE}\n\n::: {#cell-fig-annPlotLog .cell execution_count=3}\n\n::: {.cell-output .cell-output-display}\n![Graph of y=lox(x)](ANN-learning_files/figure-html/fig-annplotlog-output-1.png){#fig-annplotlog width=587 height=429}\n:::\n:::\n\n\n위의 그래프(@fig-annPlotLog )에서 보듯 $x$가 1일때 $y$는 0이 되고 $x$가 0에 근접시 $y$는 점점 작아집니다. 교차 엔트로피는 여기에 음(`-`)의 부호를 붙여 활률값인 `x`가 작아질수록 손실값인 `y`가 크게 나오도록 하였습니다.\n\n위의 식(@eq-annCEE )을 코드로 구현해 보겠습니다.\n\n::: {#bf40b27b .cell execution_count=4}\n``` {.python .cell-code}\n# 정답 레이블은 2\nt = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n\ndef corss_entropy_error(y, t):\n  delta = 1e-7 # <1>\n  return -np.sum(t * np.log(y + delta)) # <1>\n\n# 예1: 2일 확률이 제일 높음(60%)\ny = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\nl1 = corss_entropy_error(np.array(y), np.array(t))\n\n# 예2: 7일 확률이 제일 높음(60%)\ny = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]\nl2 = corss_entropy_error(np.array(y), np.array(t))\n\nprint(\"1번예제: %3f, 2번예제:%3f\" % (l1, l2))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1번예제: 0.510825, 2번예제:2.302584\n```\n:::\n:::\n\n\n1. `delta`를 설정한 이유는 `y`가 `0`이면 `np,log`함수는 무한대인 `inf`값을 출력하는데 이를 방지하기 위함\n\n1번 예제의 손실값이 `0.51`로 2번 예제의 손실값보다 낮은 값을 출력하여 오차제곱합의 결과와 일치함을 확인할 수 있습니다.\n\n### CEE with Mini-batch\n\n손실함수는 주어진 데이터에 대한 모든 손실값의 합을 모델의 평가지료로 산출합니다. 가령 훈련 데이터가 100개이 60,000개 인경우 60,000개의 손실값을 산출해야 합니다.\n\n이렇게 훈련데이터 모두에 대한 손실값을 산출하는 경우 데이터가 증가할 수록 산출시간이 오래 걸리는 문제가 있습니다. 그렇다면 이를 보다 효율적으로 할 수 있는 방법은 무엇이 있을까요?\n\n일부 데이터를 추려 **근사치**를 계산하는 방법을 생각할 수 있습니다. 이러한 방법을 **미니배치**(mini-batch)라고 하며 훈련데이터 전체에서 임의로 특정 데이터를 뽑아 학습하는 방법입니다.\n\n우리는 이러한 **미니배치 학습**을 위해 몇개를 임의 추출할지(`batch-size`)와 이러한 과정을 몇번 수행할 것인지(`number of iteration`)에 대한 고민을 해야 합니다, 그 이유는 배치는 전체데이터의 일부만을 대상으로 하기 때문입니다.^[본 문제는 hyper parameter의 설정에 대한 내용으로 overfitting을 방지하려는 문제와 관련이 있습니다.]\n\n::: {#46da539f .cell execution_count=5}\n``` {.python .cell-code}\nimport sys, os\nsys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\nfrom dataset.mnist import load_mnist\n\n# MNIST 데이터셋 로드\n(x_train, t_train), (x_test, t_test) = load_mnist(normalize = True, one_hot_label = True)\n\n# Mini-batch 설정\ntrain_size = x_train.shape[0]\nbatch_size = 10\nbatch_mask = np.random.choice(train_size, batch_size) # <1>\n\n# Batch용 데이터 설정\nx_batch = x_train[batch_mask] # <2>\nt_batch = t_train[batch_mask] # <2>\n```\n:::\n\n\n1. `np.randon.choice`는 전체 N개의 데이터(`train_size`)에서 임의로 몇개(`batch_size`)를 추출할 것인지 결정\n2. `batch_mask`는 임의 뽑힌 데이터의 인덱스 값\n\n위에서 우리는 학습을 효율적으로 실시하기 위하여 미니배치를 설정하였습니다. 그렇다면 미니배치로 손실을 어떻게 구해야 할까고민입니다. 앞서 구현한 교차 엔트로피 오차를 아래의 코드와 같이 일부만 수정하면 간단히 해결할 수 있습니다.\n\n::: {#ecf41d4f .cell execution_count=6}\n``` {.python .cell-code}\ndef corss_entropy_error(y, t):\n  if y.ndim == 1:\n    t = t.reshape(1, t.size)\n    y = y.reshape(1, y.size)\n  \n  batch_size = y.shape[0]\n  return -np.sum(t * np.log(y + 1e-7)) / batch_size                            # <1>\n  return -np.sum(t * np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size  # <2>\n```\n:::\n\n\n1. 정답 레이블이 **원-핫 인코딩** 형상인 경우\n2. 정답 레이블이 **숫자 레이블**인 경우\n\n## Gradient Descent\n\n",
    "supporting": [
      "ANN-learning_files"
    ],
    "filters": [],
    "includes": {}
  }
}