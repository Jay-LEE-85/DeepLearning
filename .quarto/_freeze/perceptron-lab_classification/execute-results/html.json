{
  "hash": "890b28ae150269e7a0534d2810394666",
  "result": {
    "engine": "jupyter",
    "markdown": "# Lab: Classification with Perceptron\n\n---\n\n퍼셉트론을 활용하여 **분류**(Classification) 문제를 풀어보도록 하겠습니다. 이를 해결해 가는 과정에서 다양한 정보를 학습할 수 있기를 기대하기 때문입니다.\n\n먼저, 분류는 Supervised Learning으로 미리 리벨(Label)이 붙어 있는 데이터를 분류하는 문제입니다. 우리가 사용하는 방법은 선형적인 방법과 비선형적인 방법 등등을 살펴보겠습니다.\n\n::: {.callout-tip}\n## Lab 작성시 참고자료\n-   아래의 실습내용은 Kaist 이승철 교수님의 강의 자료를 참고하였습니다.\n-   참고자료는 [DEEP LEARNING](https://iai.postech.ac.kr/teaching/deep-learning)에서 확인할 수 있습니다.\n:::\n\n## Classification\n\n우리가 실습할 분류 문제는 **선형 판별 분석**(LDA, Linear Discriminant Analysis)입니다. 수치형 입력변수를 받아 범주형 타겟 변수를 예측하는 분류 방법으로 퍼셈트론을 활용하여 해결하기 좋은 문제 입니다.\n\n\n\n::: {#cell-fig-classification .cell execution_count=2}\n\n::: {.cell-output .cell-output-display}\n![Example of LDA](perceptron-lab_classification_files/figure-html/fig-classification-output-1.png){#fig-classification width=520 height=386}\n:::\n:::\n\n\n위 산점도(@fig-classification )와 같은 산점도에서 `C1`과 `C0`을 구분하는 문제를 퍼셉트론을 이용하여 해결해 보겠습니다.\n\n전장에서 살펴본 퍼셉트론(@fig-perceptron1)에 따라 설명하고자 합니다. 우선 입력받는 변수는 $x$, 가중치는 $\\omega$로 둘수 있습니다.\n\n$$ \n\\begin{align}\nx = \\begin{bmatrix}x_1\\\\ \\vdots\\\\ x_d \\end{bmatrix} \\qquad\n\\omega = \\begin{bmatrix}\\omega_1\\\\ \\vdots\\\\ \\omega_d \\end{bmatrix}\n\\end{align} \n$$ {#eq-perLab1}\n\n여기서 Weighted Sum의 결과가 특정 임계치($\\theta$)를 초과하는 지에 따라 **클래스1**($C_1$) 또는 **클래스0**($C_0$)으로 구분하고자 합니다.\n\n$$ \n\\begin{align}\nC_1 \\quad if\\, \\sum_{i=1}^{d}\\omega_{i}x_{i} > \\theta \\\\\nC_0 \\quad if\\, \\sum_{i=1}^{d}\\omega_{i}x_{i} < \\theta\n\\end{align} \n$$ {#eq-perLab2}\n\n\n이를 다시 Step function을 활용하여 @fig-perceptron1 의 노란색 박스($h(x)$)에 해당하는 노드를 다음의 식으로 변형하여 표현할 수 있습니다.\n\n여기서 임계치($\\theta$)는 편향에 해당하는 값으로 변형하고 이를 다시 입력변수 $\\omega_0$으로 간단하게 표현할 수 있습니다.\n\n$$ \n\\begin{align}\nh(x) &= \\text{sign} \\left(\\left( \\sum\\limits_{i=1}^{d}\\omega_ix_i \\right)- \\theta \\right) \\\\\n&= \\text{sign}\\left(\\left( \\sum\\limits_{i=1}^{d}\\omega_ix_i \\right)+ \\omega_0\\right)\n\\end{align} \n$$ {#eq-perLab3}\n\n$\\omega_0=1$로 두고 이를 벡터 형식(vector form)으로 표현하고, Step function을 Sign function으로 정의하면 아래와 같이 식을 수정할 수 있습니다.\n\n$$ \n\\begin{align}\nh(x) &= \\text{sign}(\\sum_{i=0}^{d}\\omega_{i}x_{i}) \n=\\text{sign}(\\omega^{T}x) \\\\\n\\text{sign}(x) &= \\begin{cases}\n1, &\\text{if }\\; x > 0\\\\\n0, &\\text{if }\\; x = 0\\\\\n-1, &\\text{if }\\; x < 0\n\\end{cases}\n\\end{align} \n$$ {#eq-perLab4}\n\n**Perceptron Learning Algorithm(PLA)**\n\n위의 식을 실제 퍼셉트론에 적용하기 위하여 위 산점도(@fig-classification )의 개별 값들을 활용하여 $\\text{sign}(\\omega^Tx_n) = \\hat{y}_n$을 산출하여 실제 라벨($y_n$)과 비교하며 두 라벨값이 다를 경우 $\\omega$를 업데이트 하도록 합니다.\n\n$$\n\\begin{align}\n\\omega \\leftarrow \n\\begin{cases}\n\\omega+y_nx_n & \\hat{y_n} \\neq y_n \\\\\n\\omega &\\hat{y_n} = y_n \\\\\n\\end{cases}\n\\end{align}\n$$  {#eq-perLab5}\n\n위의 절차를 Training Set을 이용하여 Iterative하게 반복하여 $\\omega$를 업데이트 하도록 하여 PLA를 수행합니다.\n\n**Perceptron Loss Function**\n\nPLA를 수행하는 과정에서 학습을 종결시키기 위하여 Loss function이 필요합니다. 산출값 $\\hat{y}_n$과 $y_n$을 비교하여 Loss들의 합($\\mathscr{L}(\\omega)$)이 0이 되도록 하는 조건으로 설정할 수 있습니다.\n\n$$\n\\begin{align}\n\\mathscr{L}(\\omega) = \\sum_{n =1}^{m} \\max \\left\\{ 0, -y_n \\cdot \\left(\\omega^T x_n \\right)\\right\\}\n\\end{align}\n$$  {#eq-perLab7}\n\n\n## Solving with perceptron algorithm\n\n위에서 살펴본 내용을 이제 코드로 작성해 보겠습니다. 기본적인 입력변수 $x$를 `x1`과 `x2`로 라벨값 $y$를 `C1`과 `C0`로 생성하였습니다. 그에 따른 산점도 데이터는 아래와 같습니다.\n\n::: {#c1a953cc .cell execution_count=3}\n``` {.python .cell-code}\n#training data gerneration\nm = 100\nx1 = 8*np.random.rand(m, 1)\nx2 = 7*np.random.rand(m, 1) - 4\n\ng = 0.8*x1 + x2 -3\n\nC1 = np.where(g >= 1)[0]\nC0 = np.where(g < -1)[0]\n```\n:::\n\n\n::: {#0d64d506 .cell execution_count=4}\n\n::: {.cell-output .cell-output-display}\n![Linearly Separable Classes with no dicision boundary](perceptron-lab_classification_files/figure-html/cell-5-output-1.png){width=520 height=386}\n:::\n:::\n\n\n각각의 입력값을 `numpy`로 연산할 수 있도록 Vectorize를 진행합니다.\n\n$$\n\\begin{align}\nx &= \\begin{bmatrix} \\left(x^{(1)}\\right)^T \\\\ \\left(x^{(2)}\\right)^T \\\\ \\left(x^{(3)}\\right)^T\\\\ \\vdots \\\\ \\left(x^{(m)}\\right)^T \\end{bmatrix} = \\begin{bmatrix} 1 & x_1^{(1)} & x_2^{(1)} \\\\ 1 & x_1^{(2)} & x_2^{(2)} \\\\ 1 & x_1^{(3)} & x_2^{(3)}\\\\\\vdots & \\vdots & \\vdots \\\\ 1 & x_1^{(m)} & x_2^{(m)}\\end{bmatrix} \\\\\ny &= \\begin{bmatrix}y^{(1)} \\\\ y^{(2)} \\\\ y^{(3)}\\\\ \\vdots \\\\ y^{(m)} \\end{bmatrix}\n\\end{align}\n$$\n\n::: {#757587f9 .cell execution_count=5}\n``` {.python .cell-code}\nX1 = np.hstack([np.ones([C1.shape[0],1]), x1[C1], x2[C1]])\nX0 = np.hstack([np.ones([C0.shape[0],1]), x1[C0], x2[C0]])\n\nX = np.vstack([X1, X0])\ny = np.vstack([np.ones([C1.shape[0],1]), -np.ones([C0.shape[0],1])])\n\nX = np.asmatrix(X)\ny = np.asmatrix(y)\n```\n:::\n\n\n가중치 $\\omega$를 1로 초깃값을 설정하고, 각각의 입력값을 이용하여 PLA를 실행하며 가중치 업데이트($\\omega \\leftarrow \\omega + yx$)를 실행합니다.\n\n::: {#eef20e74 .cell execution_count=6}\n``` {.python .cell-code}\nw = np.ones([3,1])\nw = np.asmatrix(w)\n\nn_iter = y.shape[0]\nflag = 0\n\nwhile flag == 0:\n    flag = 1\n    for i in range(n_iter):\n        if y[i,0] != np.sign(X[i,:]*w)[0,0]:\n            w += y[i,0]*X[i,:].T\n            flag = 0\n```\n:::\n\n\n위의 절차를 실행하여 산출한 결정 경계(Dicision Boundary)는 아래와 같습니다. \n\n::: {#0a816fd9 .cell execution_count=7}\n\n::: {.cell-output .cell-output-display}\n![Linearly Separable Classes with dicision boundary](perceptron-lab_classification_files/figure-html/cell-8-output-1.png){width=524 height=363}\n:::\n:::\n\n\n::: {.callout-warning}\n## Perceptron과 최적화의 문제\n-   여기서 중요한 것은 퍼셉트론은 구분하는 경계 산출에 촛점을 두고 있으므로 우리가 예상하는 최적 경계에 해당하지 않을 수 있습니다.\n-   가령 최적 경계는 두 그룹의 중간쯤 위치해야 할 것으로 생각이 됩니다. 그래야 향후 입력값이 추가 되었을 때 경계가 유효할 가능성이 높기 때문입니다.\n-   이를 해결하기 위하여는 내적을 통한 거리의 개념이 들어가고, Loss Function도 Logistic Regression의 방법으로 해결해야 하지 않을까라고 예상할 수 있습니다.\n:::\n\n",
    "supporting": [
      "perceptron-lab_classification_files"
    ],
    "filters": [],
    "includes": {}
  }
}