{
  "hash": "303018144ff38c46b0ab955799170810",
  "result": {
    "engine": "jupyter",
    "markdown": "# Lab: Classification with Perceptron\n\n---\n\n퍼셉트론을 활용하여 **분류**(Classification) 문제를 풀어보도록 하겠습니다. 이를 해결해 가는 과정에서 다양한 정보를 학습할 수 있기를 기대하기 때문입니다.\n\n먼저, 분류는 Supervised Learning으로 미리 리벨(Label)이 붙어 있는 데이터를 분류하는 문제입니다. 우리가 사용하는 방법은 선형적인 방법과 비선형적인 방법 등등을 살펴보겠습니다.\n\n::: {.callout-tip}\n## Lab 작성시 참고자료\n-   아래의 실습내용은 Kaist 이승철 교수님의 강의 자료를 참고하였습니다.\n-   참고자료는 [DEEP LEARNING](https://iai.postech.ac.kr/teaching/deep-learning)에서 확인할 수 있습니다.\n:::\n\n## Classification\n\n우리가 실습할 분류 문제는 **선형 판별 분석**(Linear Discriminant Analysis)입니다. 수치형 입력변수를 받아 범주형 타겟 변수를 예측하는 분류 방법으로 퍼셈트론을 활용하여 해결하기 좋은 문제 입니다.\n\n\n\n::: {#cell-fig-classification .cell execution_count=2}\n\n::: {.cell-output .cell-output-display}\n![Example of LDA](perceptron-lab_classification_files/figure-html/fig-classification-output-1.png){#fig-classification width=520 height=386}\n:::\n:::\n\n\n위 산점도(@fig-classification )와 같은 산점도에서 `C1`과 `C0`을 구분하는 문제를 퍼셉트론을 이용하여 해결해 보겠습니다.\n\n전장에서 살펴본 퍼셉트론(@fig-perceptron1)에 따라 설명하고자 합니다. 우선 입력받는 변수는 $x$, 가중치는 $\\omega$로 둘수 있습니다.\n\n$$ \n\\begin{align}\nx = \\begin{bmatrix}x_1\\\\ \\vdots\\\\ x_d \\end{bmatrix} \\qquad\n\\omega = \\begin{bmatrix}\\omega_1\\\\ \\vdots\\\\ \\omega_d \\end{bmatrix}\n\\end{align} \n$$ {#eq-perLab1}\n\n여기서 Weighted Sum의 결과가 특정 임계치($\\theta$)를 초과하는 지에 따라 **클래스1**($C_1$) 또는 **클래스0**($C_0$)으로 구분하고자 합니다.\n\n$$ \n\\begin{align}\nC_1 \\quad if\\, \\sum_{i=1}^{d}\\omega_{i}x_{i} > \\theta \\\\\nC_0 \\quad if\\, \\sum_{i=1}^{d}\\omega_{i}x_{i} < \\theta\n\\end{align} \n$$ {#eq-perLab2}\n\n\n이를 다시 Step function을 활용하여 @fig-perceptron1 의 노란색 박스($h(x)$)에 해당하는 노드를 다음의 식으로 변형하여 표현할 수 있습니다.\n\n여기서 임계치($\\theta$)는 편향에 해당하는 값으로 변형하고 이를 다시 입력변수 $\\omega_0$으로 간단하게 표현할 수 있습니다.\n\n$$ \n\\begin{align}\nh(x) &= \\text{sign} \\left(\\left( \\sum\\limits_{i=1}^{d}\\omega_ix_i \\right)- \\theta \\right) \\\\\n&= \\text{sign}\\left(\\left( \\sum\\limits_{i=1}^{d}\\omega_ix_i \\right)+ \\omega_0\\right)\n\\end{align} \n$$ {#eq-perLab3}\n\n$\\omega_0=1$로 두고 이를 벡터 형식(vector form)으로 표현하고, Step function을 Sign function으로 정의하면 아래와 같이 식을 수정할 수 있습니다.\n\n$$ \n\\begin{align}\nh(x) &= \\text{sign}(\\sum_{i=0}^{d}\\omega_{i}x_{i}) \n=\\text{sign}(\\omega^{T}x) \\\\\n\\text{sign}(x) &= \\begin{cases}\n1, &\\text{if }\\; x > 0\\\\\n0, &\\text{if }\\; x = 0\\\\\n-1, &\\text{if }\\; x < 0\n\\end{cases}\n\\end{align} \n$$ {#eq-perLab4}\n\n**Perceptron Learning Algorithm(PLA)**\n\n위의 식을 실제 퍼셉트론에 적용하기 위하여 위 산점도(@fig-classification )의 개별 값들을 활용하여 $\\text{sign}(\\omega^Tx_n) = \\hat{y}_n$을 산출하여 실제 라벨($y_n$)과 비교하며 두 라벨값이 다를 경우 $\\omega$를 업데이트 하도록 합니다.\n\n$$\n\\begin{align}\n\\omega \\leftarrow \n\\begin{cases}\n\\omega+y_nx_n & \\hat{y_n} \\neq y_n \\\\\n\\omega &\\hat{y_n} = y_n \\\\\n\\end{cases}\n\\end{align}\n$$  {#eq-perLab5}\n\n위의 절차를 Training Set을 이용하여 Iterative하게 반복하여 $\\omega$를 업데이트 하도록 하여 PLA를 수행합니다.\n\n**Perceptron Loss Function**\n\nPLA를 수행하는 과정에서 학습을 종결시키기 위하여 Loss function이 필요합니다. 산출값 $\\hat{y}_n$과 $y_n$을 비교하여 Loss들의 합($\\mathscr{L}(\\omega)$)이 0이 되도록 하는 조건으로 설정할할 수 있습니다.\n\n$$\n\\begin{align}\n\\mathscr{L}(\\omega) = \\sum_{n =1}^{m} \\max \\left\\{ 0, -y_n \\cdot \\left(\\omega^T x_n \\right)\\right\\}\n\\end{align}\n$$  {#eq-perLab7}\n\n\n## Solving with perceptron algorithm\n\n::: {#2c93600e .cell execution_count=3}\n``` {.python .cell-code}\n# load library\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n#training data gerneration\nm = 100\nx1 = 8*np.random.rand(m, 1)\nx2 = 7*np.random.rand(m, 1) - 4\n\ng = 0.8*x1 + x2 - 3 # <1>\n\nC1 = np.where(g >= 1)[0]\nC0 = np.where(g < -1)[0]\n```\n:::\n\n\n1. test\n\n::: {#884d51cf .cell execution_count=4}\n\n::: {.cell-output .cell-output-display}\n![](perceptron-lab_classification_files/figure-html/cell-5-output-1.png){width=520 height=386}\n:::\n:::\n\n\n::: {#56f66589 .cell execution_count=5}\n``` {.python .cell-code}\nX1 = np.hstack([np.ones([C1.shape[0],1]), x1[C1], x2[C1]])\nX0 = np.hstack([np.ones([C0.shape[0],1]), x1[C0], x2[C0]])\nX = np.vstack([X1, X0])\n\ny = np.vstack([np.ones([C1.shape[0],1]), -np.ones([C0.shape[0],1])])\n\nX = np.asmatrix(X)\ny = np.asmatrix(y)\n```\n:::\n\n\n::: {#78eac6cd .cell execution_count=6}\n``` {.python .cell-code}\nw = np.ones([3,1])\nw = np.asmatrix(w)\n\nn_iter = y.shape[0]\nflag = 0\n\nwhile flag == 0:\n    flag = 1\n    for i in range(n_iter):\n        if y[i,0] != np.sign(X[i,:]*w)[0,0]:\n            w += y[i,0]*X[i,:].T\n            flag = 0\n\nx1p = np.linspace(0,8,100).reshape(-1,1)\nx2p = - w[1,0]/w[2,0]*x1p - w[0,0]/w[2,0]\n```\n:::\n\n\n::: {#eeb173e9 .cell execution_count=7}\n\n::: {.cell-output .cell-output-display}\n![](perceptron-lab_classification_files/figure-html/cell-8-output-1.png){width=524 height=363}\n:::\n:::\n\n\n",
    "supporting": [
      "perceptron-lab_classification_files\\figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}