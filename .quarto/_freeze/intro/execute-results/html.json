{
  "hash": "38b5288514de11b9435e12c8e2d647ea",
  "result": {
    "engine": "jupyter",
    "markdown": "# Introduction {.unnumbered}\n\n---\n\nDeeplearning(이하 'DL')은 다양한 분야에 활용되고 있습니다. 그러나 DL이 무엇인지? 그리고 DL이 어떻게 작동하는지? 필자들은 이러한 기초적인 물음에 답을 찾고 싶었습니다. \n\n그러나 시중에 나와 있는 대부분의 문헌은 DL의 Application에 촛점을 두고 다양한 Framework(`tensorflow`, `torch` 등)의 사용례에 집중하여 DL의 Fundation을 이해할 수 없는 문제를 접하게 되었었습니다.\n\nDL을 Deep하게 이해하기 위하여 필자들은 기초부터 시작하여 응용까지 모든 과정을 따라가 보기위하여 본 프로젝트를 시작하였습니다. \n\n이 과정은 쉽지 않을 것이지만 다양한 이론과 수식들을 직접 찾아보고 이해해보고, 동시에 외부 라이브러리에 의존하지 않고 기초적인 내용을 하나 하나를 구현해 봄으로써 더 Deep하게 DL을 이해하고, 향후 Framework의 활용시 높은 수준의 이해를 갖고 새로운 문제들을 해결하고자 합니다.\n\n## Learning Path\n\n필자들의 DL 학습배경에 따라 아래의 경로로 DL을 학습할 계획이며 많은 문헌에서 다루고 있는 학습경로 입니다.\n\n학습경로가 정확한지? 적절한지? 알 수 없지만, 적어도 *A to Z*의 관점에서 빠짐없이 모든 내용을 학습해보기로 하였습니다.\n\n학습경로에 관한 내용은 @fig-LearningPath 을 참고하기 바라며, 학습을 수행하는 과정마다 변경되거나 추가되는 사항은 지속적으로 반영해 나갈 예정입니다.\n\n\n```{mermaid}\n%%| label: fig-LearningPath\n%%| fig-cap: \"Learning Path\"\n\nflowchart LR\n  per[Perceptron]:::ch --> per1[Classification]\n  per --> per2[Logistic Regression]\n\n  per1 --> per_l1{{Lab1: Classification}}:::lab\n\n  ann(Artificial Neural Net):::ch --> ann1[Intro of ANN] & ann2[Activation Function] & ann3[Forward Propagation] & ann4[Learning Process] & ann5[Back Propagation]\n\n  ann1 & ann2 & ann3 & ann4 --> ann_l{{Lab2: Classification with MNIST}}:::lab\n\n  classDef ch fill:#ccccff\n  classDef lab fill:#ccffcc\n```\n\n## Environment\n\n우리가 학습하며 사용한 실습환경에 대하여 간단하게 소개하겠습니다. \n\n누구나 접근이 가능한 `Python`을 기반으로 하고 있고, 실습에 사용하는 라이브러리(@tbl-packages )는 의존도를 최소화 하기 위하여 `numpy`를 주로 사용하였습니다. 그리고 실습결과를 도식화하기 위하여는 `matplotlib`을 사용하였습니다.\n\n이론에 대한 충분한 실습을 완료한 뒤에는 `tensorflow` 또는 `torch`를 사용하기로 하였습니다. 이는 **NVIDIA**의 GPU를 활용하여 보다 Deep한 신경망을 구현하기 위함임을 참고하여 주시고 학습과정에서 본 Framework의 사용은 최소화 할 예정입니다.\n\n학습경로와 마찬가지로 아래의 테이블에 적시된 라이브러리와 그 버전은 수시로 업데이트 할 예정입니다.\n\n| Name | Version |\n|---|---|\n| `numpy` | #.#.# |\n| `matplotlib` | #.#.# |\n| `tensorflow` | #.#.# |\n| `torch` | #.#.# |\n\n: List of Packages {#tbl-packages}\n\n## How to read\n\n이 사이트에서 다루는 내용은 기본적인 이론에 대한 설명과 이와 관련된 코드와 그 실행 결과 들을 보여줄 것입니다. \n\n\n**Code Example**\n\n-   기본적으로 코드는 아래의 **Code Block**에서 모든 내용을 표시하였습니다.\n-   특별히 중요하거나 추가적인 설명이 필요한 경우 **Code Annotation**에 표시하였습니다.\n\n::: {#lst-codeExample lst-cap=\"Code Block\"}\n\n::: {#3cc56286 .cell execution_count=1}\n``` {.python .cell-code}\ndef add(num1, num2):\n  result = num1 + num2 # <1>\n  return  result\n```\n:::\n\n\n1. `num1`과 `num2`를 더하여 `result`에 할당\n:::\n\n\n**Equation Example**\n\n-   수식 중 설명이 필요한 경우는 기본적으로 본문에 내용을 표시하였습니다.\n-   설명이 완료된 수식 중 참고할 사항은 `margin`컬럼에 표시였습니다.\n\n::: {#lst-equationExample lst-cap=\"Equation\"}\n$$\\frac{d}{dx}\\left( \\int_{a}^{x} f(u)\\,du\\right)=f(x).$$\n:::\n\n::: {.column-margin}\nWe know from *the first fundamental theorem of calculus* that for $x$ in $[a, b]$:\n\n$$\\frac{d}{dx}\\left( \\int_{a}^{x} f(u)\\,du\\right)=f(x).$$\n:::\n\n\n**Callout Example**\n\n-   학습을 진행해 가는 과정에서 나오는 이슈사항은 `Callout`으로 표시해 두었습니다,\n-   각 `Callout`이 담아야 할 내용은 아래를 참고하여 주시기 바랍니다.\n\n::: {#lst-calloutExample lst-cap=\"Callout\"}\n\n::: {.callout-note}\n## Note의 활용법\n-   본문의 내용과 직접관련된 내용으로 부가적인 설명을 담고 있습니다.\n-   관련 문헌이나 자료들에서 중요한 부분을 발췌한 내용을 담고 있습니다.\n-   필자가 보다 효율적이라고 판단한 내용들을 보여주고자 합니다.\n:::\n\n::: {.callout-tip}\n## Tip의 활용법\n-   본문의 내용과 직접관련 없지만 알아두면 좋은 내용을 담고 있습니다.\n-   코드의 작성방법 등 유용한 정보를 답고 있습니다.\n-   실습과정에서 발견한 문제의 해결방법을 보여주고자 합니다.\n:::\n\n::: {.callout-warning}\n## Warning의 활용법법\n-   이해하기 어려운 내용에 대하여 그 문제를 적시하고자 합니다.\n-   실습과정에서 경험한 문제 및 해결되지 않은 오류 등을 적시하고자 합니다.\n-   해결이 완료된 경우 `note` 또는 `tip`으로 전환될 수 있습니다.\n:::\n\n:::\n\n",
    "supporting": [
      "intro_files"
    ],
    "filters": [],
    "includes": {}
  }
}