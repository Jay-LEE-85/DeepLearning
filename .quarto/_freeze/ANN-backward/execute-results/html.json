{
  "hash": "8fcf7ee5286fa2817e822c5d82e16368",
  "result": {
    "engine": "jupyter",
    "markdown": "# Backward Propagation\n\n---\n\n지금까지 순전파를 거쳐 산출한 손실함수값을 수치미분을 이용하여 기울기만큼 매개변수를 업데이트 하는 학습과정을 살펴보았습니다. 구현은 단순할 수 있으나, 계산속도는 그리 빠르지 않습니다. 여기서 등장하는 것이 **오차역전파**(backpropagation)입니다.\n\n오차역전파를 이해하기 위하여는 우리는 2가지를 먼저 이해할 필요가 있다고 생각합니다. 하나는 미분에서의 **Chain-rule**이고 다른 하나는 피보나치 수열에 기반한 **Dynamic Programming**입니다,\n\n## Pre-requisite: Chain-rule and Dynamic Programming\n\n오차역전파는 기본적으로 손실함수 값에서 입력값까지의 매개변수를 역으로 조정하는 과정으로 순방향과 반대방향으로 국소적인 미분값을 곱하며 가중치를 조정하는 절차로 이해할 수 있습니다.\n\n### Chain-rule\n\n**반대방향으로 국소적인 미분값을 곱한다**는 것은 어떠한 의미를 갖고 있을까요? 그리고 이러한 연산의 기반이 되는 개념이 무엇일까요? 이물음에 대한 답변이 바로 **연쇄법칙**입니다. 연쇄법칙에 관한 자세한 설명은 아래 **3Blue1Brown**^[3Blue1Brown은 많은 수학적인 문제를 도식화하여 아주 직관적으로 설명하고 있어 매우 유용합니다.] 영상을 참고하기 바랍니다.\n\n{{< video https://www.youtube.com/embed/tIeHLnjs5U8?rel=0 >}}\n\n### Dynamic Programming\n\n연쇄법칙을 정확히 이해하였다면 지속적으로 미분값이 재귀적으로 사용됨을 확인할 수 있습니다. 그러나 이미 계산된 미분값을 따로 저장하였다가 불러오기만 한다면 연산이 얼마나 쉬워질까요? 쉬워진다기보다 간단해지고 컴퓨터의 연산의 수를 줄일 수 있지 않을까요?\n\n바로 이러한 배경에서 연쇄법칙을 빠르게 수행하기 위하여 고려되는 방법이 **동적계획법** 입니다. 이는 피보나치 수열의 계산에 있어서 재귀적으로 반복계산되는 노드를 따로 저장하여 그 값을 호출하여 사용하도록 하므로써 연산의 수를 줄여 알고리즘의 성능을 개선해줄수 있을 것입니다.\n\n동적계획법에 대한 자세한 설명은 아래 영상을 참고하시기 바랍니다.\n\n{{< video https://www.youtube.com/embed/oBt53YbR9Kk >}}\n\n## Backpropagation\n\n역전파 과정을 연쇄법칙을 수식 및 그래프를 활용하면 보다 직관적이고 쉽게 이해할 수 있습니다. 우선 합성함수^[합성함수의 미분은 함성함수를 구성하는 각 함수의 미분의 곱으로 표현가능] @eq-annBack1 의 식을 미분을 실행하며 예로 살펴보겠습니다,\n\n$$\n\\begin{align}\nz &= t^2 \\\\\nt &= x+y\n\\end{align}\n$$ {#eq-annBack1}\n\n$x$에 대한 $z$의 미분인 $\\frac{\\partial{z}}{\\partial{x}}$은 $\\frac{\\partial{z}}{\\partial{t}}$과 $\\frac{\\partial{t}}{\\partial{x}}$의 곱으로 나타낼 수 있습니다. 그리고 $\\partial{t}$를 서로 지울 수 있습니다.\n\n$$\n\\begin{align}\n\\frac{\\partial{z}}{\\partial{x}} &= \\frac{\\partial{z}}{\\partial{t}}\\frac{\\partial{t}}{\\partial{x}} \\\\\n&= \\frac{\\partial{z}}{\\not{\\partial{t}}}\\frac{\\not{\\partial{t}}}{\\partial{x}}\n\\end{align}\n$$ {#eq-annBack2}\n\n연쇄법칙을 써서 $\\frac{\\partial{z}}{\\partial{x}}$를 구하기 위하여 편미분을 실시하고, 두 미분값을 곱하여 최종 미분값을 산출합니다.\n\n$$\n\\begin{align}\n\\frac{\\partial{z}}{\\partial{t}} &= 2t \\qquad\n\\frac{\\partial{t}}{\\partial{x}} = 1 \\\\ \\\\\n\\frac{\\partial{z}}{\\partial{x}} &= \\frac{\\partial{z}}{\\partial{t}}\\frac{\\partial{t}}{\\partial{x}} = \n2t \\cdot 1 = 2(x+y)\n\\end{align}\n$$ {#eq-annBack3}\n\n@eq-annBack3 를 그래프로 나타내어 연쇄법칙을 나타내봅시다. @fig-annBack1 에서 보는 바와 같이 오른쪽에서 왼쪽으로 신호를 전달(전파) 합니다. 전파의 과정에서 입력값에 해당하는 편미분값을 곱하여 다음 노드에 전달함을 확인 할 수 있습니다.\n\n![Multiplying the partial derivative of @eq-annBack3 and passing it on](image/fig-annBack1.png){#fig-annBack1 width=70%}\n\n@fig-annBack1 과 같은 과정에 @eq-annBack3 의 미분값을 대입하면 @fig-annBack2 와 같은 결과를 얻을 수 있습니다.\n\n![The process of showing backpropagation results](image/fig-annBack2.png){#fig-annBack2 width=70%}\n\n### Backpropagation of Addition Nodes\n\n먼저 $z=x+y$를 갖고 덧셈노드에 대한 역전파를 살펴보겠습니다. 먼저 이 식에 대한 미분을 해석적으로 구하면 $\\frac{\\partial{z}}{\\partial{x}}$와 $\\frac{\\partial{z}}{\\partial{y}}$ 모두 1이 됩니다.\n\n@fig-annBack4 의 그래프를 기준으로 역전파^[역전파는 순방향과 반대방향으로 국소적 미분(편미분)값을 곱하는 방법으로 수행] 과정을 살펴봅시다. \n\n상류에서 산출한 편미분 값($\\frac{\\partial{L}}{\\partial{z}}$)을 $x$간선의 경우 $\\frac{\\partial{L}}{\\partial{z}}\\frac{\\partial{z}}{\\partial{x}}$의 꼴로 역전파가 이루어 지고, $y$간선의 경우 $\\frac{\\partial{L}}{\\partial{z}}\\frac{\\partial{z}}{\\partial{y}}$의 꼴로 역전파가 이루어 집니다.\n\n@fig-annBack4 는 덧셈의 역전파 이므로 $\\frac{\\partial{L}}{\\partial{z}}\\frac{\\partial{z}}{\\partial{x}}=\\frac{\\partial{L}}{\\partial{z}}\\cdot1$과 $\\frac{\\partial{L}}{\\partial{z}}\\frac{\\partial{z}}{\\partial{y}}=\\frac{\\partial{L}}{\\partial{z}}\\cdot1$로 변형됨으로 입력된 값 그대로 다음 노드에 전달되게 됩니다.\n\n![Examples: Backpropagation of Addition Nodes](image/annBack3.png){#fig-annBack4 width=70%}\n\n### Backpropagation of Multiplication Nodes\n\n다음으로 $z=xy$를 갖고 곱셈노드에 대한 역전파를 살펴보겠습니다. 먼저 이 식에 대한 미분을 해석적으로 구하면 $\\frac{\\partial{z}}{\\partial{x}} = y$와 $\\frac{\\partial{z}}{\\partial{y}} = x$가 됩니다.\n\n@fig-annBack5 의 그래프를 기준으로 역전파 과정을 살펴봅시다. \n\n상류에서 산출한 편미분 값($\\frac{\\partial{L}}{\\partial{z}}$)을 $x$간선의 경우 $\\frac{\\partial{L}}{\\partial{z}}\\frac{\\partial{z}}{\\partial{x}}=\\frac{\\partial{L}}{\\partial{z}}\\frac{\\partial{z}}{\\partial{x}}=\\frac{\\partial{L}}{\\partial{z}}\\cdot y$의 꼴로 역전파가 이루어 지고, $y$간선의 경우 $\\frac{\\partial{L}}{\\partial{z}}\\frac{\\partial{z}}{\\partial{y}}=\\frac{\\partial{L}}{\\partial{z}}\\frac{\\partial{z}}{\\partial{y}}=\\frac{\\partial{L}}{\\partial{z}}\\cdot x$의 꼴로 역전파가 이루어 집니다.\n\n@fig-annBack5 는 곱셈의 역전파는 서로 바꾼값을 곱하여 하류로 흘려 보내게 됨을 확인할 수 있습니다.\n\n![Examples: Multiplication of Addition Nodes](image/annBack5.png){#fig-annBack5 width=70%}\n\n::: {.callout-tip}\n\n## 편미분과 그라디언트\n\n-   신경망의 학습을 위한 역전파 과정은 모든 독립변수에 대한 편미분을 통하여 기울기를 산출\n-   대부분의 경우 이를 간단하게 표현하기 위하여 $\\nabla$연산자를 사용\n-   편미분을 통한 가중치($\\omega$)의 업데이트 과정을 아래와 같이 표현\n\n$$\n\\omega = \\leftarrow \\omega - \\alpha \\nabla_{\\omega} \\epsilon\n$$\n\n:::\n\n## Implementing the Activation Layer\n\n역전파의 과정을 활성화 함수에 적용하여 구현해 보도록 하겠습니다. 우리가 사용할 활성화 함수는 ReLU와 Sigmoid입니다.\n\n### ReLU\n\nReLU(Rectified Linear Unit) 함수는 0을 기점으로 입력값이 0이하이면 0을 출력하고 0을 초과하면 그대로 출력하도록 하는 활성화 함수 입니다.^[$\\frac{\\partial{L}}{\\partial{y}} = \\begin{cases} 1 & (x >0) \\\\ 0 & (x \\leq 0) \\end{cases}$]\n\nReLU 함수의 미분값^[$y = \\begin{cases} x & (x >0) \\\\ 0 & (x \\leq 0) \\end{cases}$]의 역전파 과정은 (1) $x > 0$일때 역전파는 미분값($\\frac{\\partial{L}}{\\partial{y}}$)을 그대로 흘려보내고, (2) $x \\leq 0$일때 역전파는 미분값을 보내지 않습니다.\n\n![Backpropagation process of ReLU](image/fig-annBack3.png){#fig-annBack3}\n\n@fig-annBack3 의 과정을 참고하여 ReLU 계층을 코드로 구현하면 아래와 같습니다.\n\n::: {#a3c1d3c0 .cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\n\nclass Relu:\n  def __init__(self):\n    self.mask = None\n\n  def forward(self, x):\n    self.mask = (x<=0) # <1>\n    out = x.copy()\n    out[self.mask] = 0 # <2>\n    return out\n  \n  def backward(self, dout):\n    dout[self.mask] = 0 # <3>\n    dx = dout\n    return dx\n\nx = np.array([[1.0, -0.5], [-2.0, 3.0]])\nmask = (x <= 0)\nprint(mask)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[False  True]\n [ True False]]\n```\n:::\n:::\n\n\n1. `x`이하인 값의 경우 `True`를 반환하고 초과인 경우 `False`를 반환\n2. `self.mask`에서 `True`에 해당하는 값을 `0`으로 함\n3. `self.mask`가 `True`인 값은 역전파시 0을 산출하도록 함\n\n### Sigmoid Layer\n\nSigmoid^[$y=\\frac{1}{1+exp(-x)}$]의 경우는 곱하기, 더하기 등의 노드가 결합되는 형태로 ReLU에 비하여 조금 복잡할 수 있으나 아래의 그래프(@fig-annBack4 )를 참고하여 단계별로 나누어 역전파 과정을 설명하도록 하겠습니다.\n\n::: {#fig-annBack4 layout-nrow=2}\n\n![Forward](image/fig-annBack4.png){}\n\n![Backward](image/fig-annBack5.png){}\n\nBackpropagation process of Sigmoid\n:::\n\n계산 그래프(@fig-annBack4 ) '/'노드의 역전파를 설명을 위해 순전화 과정에서 약간의 트릭으로 역수의 곱하기 노드로 변형할 수 있습니다.. 이는 $1+epx(-x)$를 $x$로 두고 $y=\\frac{1}{x}$로 순전파를 진행하여 $y$를 출력하고, 역전파는 곱하기이므로 $x$에 대한 $y$의 미분값^[$\\frac{\\partial{y}}{\\partial{x}}=-\\frac{1}{x^2}=-y^2$]을 입력값($\\frac{\\partial{L}}{\\partial{y}}$)과 곱하여 하류로 흘려보내면 $\\times$ 노드의 역전파로 풀이할 수 있습니다.\n\n`+`노드는 상류의 값을 하류로 그대로 흘려 보내는 것으로 `/`노드에서 산출한 $-\\frac{\\partial{L}}{\\partial{y}}y^2$을 그대로 하류로 흘려 보내면 되겠습니다.\n\n`exp`노드는 곱하기 노드 이므로 상류에서 흘러온 값($-\\frac{\\partial{L}}{\\partial{y}}y^2$)과 순전파시 해당노드의 산출값의 미분값($\\frac{\\partial{y}}{\\partial{x}}=exp(x)$)을 곱해야 하므로 $-\\frac{\\partial{L}}{\\partial{y}}y^2exp(-x)$를 하류로 흘려보내게 됩니다.\n\n마지막으로 $\\times$으 노드이다. 상류에서 입력되는 값과 해당노드의 미분값을 곱하여 하류로 흘려 보내는 방식으로 역전파를 수행하며, 곱하기 노드의 순전파시 산출이 $-x$이므로 역전파에 사용할 미분값은 $-1$이므로 입력된 값의 부호만을 변경해주면 됩니다.\n\nSigmoid 노드의 역전파를 단순화 하면 입력값은 $\\frac{\\partial{L}}{\\partial{y}}$이 되고, 출력값은 $\\frac{\\partial{L}}{\\partial{y}}y^2exp(-x)$이 되게 됩니다. 이 식을 다음(@eq-annBack4)과 같이 변형해서 @fig-annBack6 과 같이 최종 단순화 할 수 있습니다.\n\n$$\n\\begin{align}\n\\frac{\\partial{}}{\\partial{}} &= \\frac{\\partial{}}{\\partial{}}\\frac{}{}exp(-x) \\\\\n&= \\frac{\\partial{}}{\\partial{}}\\frac{}{}\\frac{}{} \\\\\n&= \\frac{\\partial{}}{\\partial{}}y(1-y)\n\\end{align}\n$$\n\n![Simplified Backward of Sigmoid](image/fig-annBack6.png){#fig-annBack6}\n\n\n## Implementing the Output Layer\n\n### Affine\n\n### Softmax\n\n",
    "supporting": [
      "ANN-backward_files"
    ],
    "filters": [],
    "includes": {}
  }
}