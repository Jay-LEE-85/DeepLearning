{"title":"Backward Propagation","markdown":{"headingText":"Backward Propagation","containsRefs":false,"markdown":"---\n\n지금까지 순전파를 거쳐 산출한 손실함수값을 수치미분을 이용하여 기울기만큼 매개변수를 업데이트 하는 학습과정을 살펴보았습니다. 구현은 단순할 수 있으나, 계산속도는 그리 빠르지 않습니다. 여기서 등장하는 것이 **오차역전파**(backpropagation)입니다.\n\n오차역전파를 이해하기 위하여는 우리는 2가지를 먼저 이해할 필요가 있다고 생각합니다. 하나는 미분에서의 **Chain-rule**이고 다른 하나는 피보나치 수열에 기반한 **Dynamic Programming**입니다,\n\n## Pre-requisite: Chain-rule and Dynamic Programming\n\n오차역전파는 기본적으로 손실함수 값에서 입력값까지의 매개변수를 역으로 조정하는 과정으로 순방향과 반대방향으로 국소적인 미분값을 곱하며 가중치를 조정하는 절차로 이해할 수 있습니다.\n\n### Chain-rule\n\n**반대방향으로 국소적인 미분값을 곱한다**는 것은 어떠한 의미를 갖고 있을까요? 그리고 이러한 연산의 기반이 되는 개념이 무엇일까요? 이물음에 대한 답변이 바로 **연쇄법칙**입니다. 연쇄법칙에 관한 자세한 설명은 아래 **3Blue1Brown**^[3Blue1Brown은 많은 수학적인 문제를 도식화하여 아주 직관적으로 설명하고 있어 매우 유용합니다.] 영상을 참고하기 바랍니다.\n\n{{< video https://www.youtube.com/embed/tIeHLnjs5U8?rel=0 >}}\n\n### Dynamic Programming\n\n연쇄법칙을 정확히 이해하였다면 지속적으로 미분값이 재귀적으로 사용됨을 확인할 수 있습니다. 그러나 이미 계산된 미분값을 따로 저장하였다가 불러오기만 한다면 연산이 얼마나 쉬워질까요? 쉬워진다기보다 간단해지고 컴퓨터의 연산의 수를 줄일 수 있지 않을까요?\n\n바로 이러한 배경에서 연쇄법칙을 빠르게 수행하기 위하여 고려되는 방법이 **동적계획법** 입니다. 이는 피보나치 수열의 계산에 있어서 재귀적으로 반복계산되는 노드를 따로 저장하여 그 값을 호출하여 사용하도록 하므로써 연산의 수를 줄여 알고리즘의 성능을 개선해줄수 있을 것입니다.\n\n동적계획법에 대한 자세한 설명은 아래 영상을 참고하시기 바랍니다.\n\n{{< video https://www.youtube.com/embed/oBt53YbR9Kk >}}\n\n## Backpropagation\n\n역전파 과정을 연쇄법칙을 수식 및 그래프를 활용하면 보다 직관적이고 쉽게 이해할 수 있습니다. 우선 합성함수^[합성함수의 미분은 함성함수를 구성하는 각 함수의 미분의 곱으로 표현가능] @eq-annBack1 의 식을 미분을 실행하며 예로 살펴보겠습니다,\n\n$$\n\\begin{align}\nz &= t^2 \\\\\nt &= x+y\n\\end{align}\n$$ {#eq-annBack1}\n\n$x$에 대한 $z$의 미분인 $\\frac{\\partial{z}}{\\partial{x}}$은 $\\frac{\\partial{z}}{\\partial{t}}$과 $\\frac{\\partial{t}}{\\partial{x}}$의 곱으로 나타낼 수 있습니다. 그리고 $\\partial{t}$를 서로 지울 수 있습니다.\n\n$$\n\\begin{align}\n\\frac{\\partial{z}}{\\partial{x}} &= \\frac{\\partial{z}}{\\partial{t}}\\frac{\\partial{t}}{\\partial{x}} \\\\\n&= \\frac{\\partial{z}}{\\not{\\partial{t}}}\\frac{\\not{\\partial{t}}}{\\partial{x}}\n\\end{align}\n$$ {#eq-annBack2}\n\n연쇄법칙을 써서 $\\frac{\\partial{z}}{\\partial{x}}$를 구하기 위하여 편미분을 실시하고, 두 미분값을 곱하여 최종 미분값을 산출합니다.\n\n$$\n\\begin{align}\n\\frac{\\partial{z}}{\\partial{t}} &= 2t \\qquad\n\\frac{\\partial{t}}{\\partial{x}} = 1 \\\\ \\\\\n\\frac{\\partial{z}}{\\partial{x}} &= \\frac{\\partial{z}}{\\partial{t}}\\frac{\\partial{t}}{\\partial{x}} = \n2t \\cdot 1 = 2(x+y)\n\\end{align}\n$$ {#eq-annBack3}\n\n@eq-annBack3 를 그래프로 나타내어 연쇄법칙을 나타내봅시다. @fig-annBack1 에서 보는 바와 같이 오른쪽에서 왼쪽으로 신호를 전달(전파) 합니다. 전파의 과정에서 입력값에 해당하는 편미분값을 곱하여 다음 노드에 전달함을 확인 할 수 있습니다.\n\n![Multiplying the partial derivative of @eq-annBack3 and passing it on](image/fig-annBack1.png){#fig-annBack1 width=70%}\n\n@fig-annBack1 과 같은 과정에 @eq-annBack3 의 미분값을 대입하면 @fig-annBack2 와 같은 결과를 얻을 수 있습니다.\n\n![The process of showing backpropagation results](image/fig-annBack2.png){#fig-annBack2 width=70%}\n\n### Backpropagation of Addition Nodes\n\n먼저 $z=x+y$를 갖고 덧셈노드에 대한 역전파를 살펴보겠습니다. 먼저 이 식에 대한 미분을 해석적으로 구하면 $\\frac{\\partial{z}}{\\partial{x}}$와 $\\frac{\\partial{z}}{\\partial{y}}$ 모두 1이 됩니다.\n\n@fig-annBack4 의 그래프를 기준으로 역전파^[역전파는 순방향과 반대방향으로 국소적 미분(편미분)값을 곱하는 방법으로 수행] 과정을 살펴봅시다. \n\n상류에서 산출한 편미분 값($\\frac{\\partial{L}}{\\partial{z}}$)을 $x$간선의 경우 $\\frac{\\partial{L}}{\\partial{z}}\\frac{\\partial{z}}{\\partial{x}}$의 꼴로 역전파가 이루어 지고, $y$간선의 경우 $\\frac{\\partial{L}}{\\partial{z}}\\frac{\\partial{z}}{\\partial{y}}$의 꼴로 역전파가 이루어 집니다.\n\n@fig-annBack4 는 덧셈의 역전파 이므로 $\\frac{\\partial{L}}{\\partial{z}}\\frac{\\partial{z}}{\\partial{x}}=\\frac{\\partial{L}}{\\partial{z}}\\cdot1$과 $\\frac{\\partial{L}}{\\partial{z}}\\frac{\\partial{z}}{\\partial{y}}=\\frac{\\partial{L}}{\\partial{z}}\\cdot1$로 변형됨으로 입력된 값 그대로 다음 노드에 전달되게 됩니다.\n\n![Examples: Backpropagation of Addition Nodes](image/annBack3.png){#fig-annBack4 width=70%}\n\n### Backpropagation of Multiplication Nodes\n\n다음으로 $z=xy$를 갖고 곱셈노드에 대한 역전파를 살펴보겠습니다. 먼저 이 식에 대한 미분을 해석적으로 구하면 $\\frac{\\partial{z}}{\\partial{x}} = y$와 $\\frac{\\partial{z}}{\\partial{y}} = x$가 됩니다.\n\n@fig-annBack5 의 그래프를 기준으로 역전파 과정을 살펴봅시다. \n\n상류에서 산출한 편미분 값($\\frac{\\partial{L}}{\\partial{z}}$)을 $x$간선의 경우 $\\frac{\\partial{L}}{\\partial{z}}\\frac{\\partial{z}}{\\partial{x}}=\\frac{\\partial{L}}{\\partial{z}}\\frac{\\partial{z}}{\\partial{x}}=\\frac{\\partial{L}}{\\partial{z}}\\cdot y$의 꼴로 역전파가 이루어 지고, $y$간선의 경우 $\\frac{\\partial{L}}{\\partial{z}}\\frac{\\partial{z}}{\\partial{y}}=\\frac{\\partial{L}}{\\partial{z}}\\frac{\\partial{z}}{\\partial{y}}=\\frac{\\partial{L}}{\\partial{z}}\\cdot x$의 꼴로 역전파가 이루어 집니다.\n\n@fig-annBack5 는 곱셈의 역전파는 서로 바꾼값을 곱하여 하류로 흘려 보내게 됨을 확인할 수 있습니다.\n\n![Examples: Multiplication of Addition Nodes](image/annBack5.png){#fig-annBack5 width=70%}\n\n::: {.callout-tip}\n\n## 편미분과 그라디언트\n\n-   신경망의 학습을 위한 역전파 과정은 모든 독립변수에 대한 편미분을 통하여 기울기를 산출\n-   대부분의 경우 이를 간단하게 표현하기 위하여 $\\nabla$연산자를 사용\n-   편미분을 통한 가중치($\\omega$)의 업데이트 과정을 아래와 같이 표현\n\n$$\n\\omega = \\leftarrow \\omega - \\alpha \\nabla_{\\omega} \\epsilon\n$$\n\n:::\n\n## Implementing the Activation Layer\n\n역전파의 과정을 활성화 함수에 적용하여 구현해 보도록 하겠습니다. 우리가 사용할 활성화 함수는 ReLU와 Sigmoid입니다.\n\n### ReLU\n\nReLU(Rectified Linear Unit) 함수는 0을 기점으로 입력값이 0이하이면 0을 출력하고 0을 초과하면 그대로 출력하도록 하는 활성화 함수 입니다.^[$\\frac{\\partial{L}}{\\partial{y}} = \\begin{cases} 1 & (x >0) \\\\ 0 & (x \\leq 0) \\end{cases}$]\n\nReLU 함수의 미분값^[$y = \\begin{cases} x & (x >0) \\\\ 0 & (x \\leq 0) \\end{cases}$]의 역전파 과정은 (1) $x > 0$일때 역전파는 미분값($\\frac{\\partial{L}}{\\partial{y}}$)을 그대로 흘려보내고, (2) $x \\leq 0$일때 역전파는 미분값을 보내지 않습니다.\n\n![Backpropagation process of ReLU](image/fig-annBack3.png){#fig-annBack3}\n\n@fig-annBack3 의 과정을 참고하여 ReLU 계층을 코드로 구현하면 아래와 같습니다.\n\n``` {python}\nimport numpy as np\n\nclass Relu:\n  def __init__(self):\n    self.mask = None\n\n  def forward(self, x):\n    self.mask = (x<=0) # <1>\n    out = x.copy()\n    out[self.mask] = 0 # <2>\n    return out\n  \n  def backward(self, dout):\n    dout[self.mask] = 0 # <3>\n    dx = dout\n    return dx\n\nx = np.array([[1.0, -0.5], [-2.0, 3.0]])\nmask = (x <= 0)\nprint(mask)\n```\n1. `x`이하인 값의 경우 `True`를 반환하고 초과인 경우 `False`를 반환\n2. `self.mask`에서 `True`에 해당하는 값을 `0`으로 함\n3. `self.mask`가 `True`인 값은 역전파시 0을 산출하도록 함\n\n### Sigmoid Layer\n\nSigmoid^[$y=\\frac{1}{1+exp(-x)}$]의 경우는 곱하기, 더하기 등의 노드가 결합되는 형태로 ReLU에 비하여 조금 복잡할 수 있으나 아래의 그래프(@fig-annBack4 )를 참고하여 단계별로 나누어 역전파 과정을 설명하도록 하겠습니다.\n\n::: {#fig-annBack4 layout-nrow=2}\n\n![Forward](image/fig-annBack4.png){}\n\n![Backward](image/fig-annBack5.png){}\n\nBackpropagation process of Sigmoid\n:::\n\n계산 그래프(@fig-annBack4 ) '/'노드의 역전파를 설명을 위해 순전화 과정에서 약간의 트릭으로 역수의 곱하기 노드로 변형할 수 있습니다.. 이는 $1+epx(-x)$를 $x$로 두고 $y=\\frac{1}{x}$로 순전파를 진행하여 $y$를 출력하고, 역전파는 곱하기이므로 $x$에 대한 $y$의 미분값^[$\\frac{\\partial{y}}{\\partial{x}}=-\\frac{1}{x^2}=-y^2$]을 입력값($\\frac{\\partial{L}}{\\partial{y}}$)과 곱하여 하류로 흘려보내면 $\\times$ 노드의 역전파로 풀이할 수 있습니다.\n\n`+`노드는 상류의 값을 하류로 그대로 흘려 보내는 것으로 `/`노드에서 산출한 $-\\frac{\\partial{L}}{\\partial{y}}y^2$을 그대로 하류로 흘려 보내면 되겠습니다.\n\n`exp`노드는 곱하기 노드 이므로 상류에서 흘러온 값($-\\frac{\\partial{L}}{\\partial{y}}y^2$)과 순전파시 해당노드의 산출값의 미분값($\\frac{\\partial{y}}{\\partial{x}}=exp(x)$)을 곱해야 하므로 $-\\frac{\\partial{L}}{\\partial{y}}y^2exp(-x)$를 하류로 흘려보내게 됩니다.\n\n마지막으로 $\\times$으 노드이다. 상류에서 입력되는 값과 해당노드의 미분값을 곱하여 하류로 흘려 보내는 방식으로 역전파를 수행하며, 곱하기 노드의 순전파시 산출이 $-x$이므로 역전파에 사용할 미분값은 $-1$이므로 입력된 값의 부호만을 변경해주면 됩니다.\n\nSigmoid 노드의 역전파를 단순화 하면 입력값은 $\\frac{\\partial{L}}{\\partial{y}}$이 되고, 출력값은 $\\frac{\\partial{L}}{\\partial{y}}y^2exp(-x)$이 되게 됩니다. 이 식을 다음(@eq-annBack4)과 같이 변형해서 @fig-annBack6 과 같이 최종 단순화 할 수 있습니다.\n\n$$\n\\begin{align}\n\\frac{\\partial{L}}{\\partial{y}}y^2exp(-x) &= \\frac{\\partial{L}}{\\partial{y}}\\frac{1}{(1+exp(-x))^2}exp(-x) \\\\\n&= \\frac{\\partial{L}}{\\partial{y}}\\frac{1}{1+exp(-x)}\\frac{exp(-x)}{1+exp(-x)} \\\\\n&= \\frac{\\partial{L}}{\\partial{}}y(1-y)\n\\end{align}\n$$\n\n![Simplified Backward of Sigmoid](image/fig-annBack6.png){#fig-annBack6  width=70%}\n\n@fig-annBack6 의 과정을 참고하여 Sigmoid 계층을 코드로 구현하면 아래와 같습니다.\n\n``` {python}\nclass Sigmoid:\n  def __init__(self):\n    self.out = None\n\n  def forward(self, x):\n    out = 1/(1+exp(-x))\n    self.out = out\n    return out # <1>\n\n  def backward(self, dout):\n    dx = dout * (1.0 - self.out) * self.out\n    return dx\n```\n1. `forward` 출력을 인스턴스 변수 `out`에 보관하였다가, `backward`연산에 사용\n\n## Implementing the Output Layer\n\nANN에서 순전파때 수행하는 행렬의 곱은 기하학에서 **어파인 변환**(affine tranformation)이라고 이를 Affine 계층이라 합니다. **Softmax** 계층은 ANN의에서 입력값을 정규화하여 출력값을 산출하는 계층입니다.\n\n### Affine\n\nANN의 순전파에서 입력값($\\textbf{X}$)과 가중치($\\textbf{W}$)의 Weighted sum에 편향($\\textbf{B}$)을 합산하여 활성화 함수를 통해 출력값을 다음 계층에 전달하게 됩니다. 이 과정에서 중요한 것은 특정 계층 또는 노드의 계산과정에서의 형상 또는 차원을 일치시켜야 한다는 점입니다.\n\n![Forward Propagation of Affine](image/fig-annBack7.png){#fig-annBack7  width=70%}\n\n@fig-annBack7 는 행렬을 기준으로 한 순전파 과정입니다. 이는 2개의 입력노드를 $\\textbf{X}$, 3개의 출력노드 $\\textbf{Y}$를 갖는 신경망^[입력노드와 출력노드간 간선에 해당하는 가중치 $2\\times3$의 $\\textbf{W}$] (@fig-annBack7_2 )을 벡터폼으로 표현한 것입니다.\n\n::: {.column-margin #fig-annBack7_2}\n``` {mermaid}\nflowchart LR\n  x1((x1)) --w11---> y1((y1)) --> sigmoid1(sigmoid) --> z1((z1))\n  x1((x1)) --w12---> y2((y2)) --> sigmoid2(sigmoid) --> z2((z2))\n  x1((x1)) --w13---> y3((y3)) --> sigmoid3(sigmoid) --> z3((z3))\n  x2((x2)) --w21---> y1((y1))\n  x2((x2)) --w22---> y2((y2))\n  x2((x2)) --w23---> y3((y3))\n```\n:::\n\n@fig-annBack7 의 각 단계별 역전파를 위하여 편미분을 진행하면 다음과 같은 식이 도출됩니다.\n\n$$\n\\begin{align}\n\\frac{\\partial{L}}{\\partial{\\textbf{X}}} &= \\frac{\\partial{L}}{\\partial{\\textbf{Y}}}\\frac{\\partial{\\textbf{Y}}}{\\partial{\\textbf{X}}} \\\\\n&= \\frac{\\partial{L}}{\\partial{\\textbf{Y}}}\\frac{\\partial{(\\textbf{X}\\cdot\\textbf{W}+\\textbf{B})}}{\\partial{\\textbf{X}}} \\\\\n&= \\frac{\\partial{L}}{\\partial{\\textbf{Y}}}\\cdot\\textbf{W}^T \\\\\\\\\n\\frac{\\partial{L}}{\\partial{\\textbf{W}}} &= \\frac{\\partial{L}}{\\partial{\\textbf{Y}}}\\frac{\\partial{\\textbf{Y}}}{\\partial{\\textbf{W}}} \\\\\n&= \\frac{\\partial{L}}{\\partial{\\textbf{Y}}}\\frac{\\partial{(\\textbf{X}\\cdot\\textbf{W}+\\textbf{B})}}{\\partial{\\textbf{W}}} \\\\\n&= \\textbf{X}^T\\cdot\\frac{\\partial{L}}{\\partial{\\textbf{Y}}}\n\\end{align}\n$$ {#eq-annBack4}\n\n@eq-annBack4 의 식을 이용하여 역전파 과정을 그리면 @fig-annBack8 과 같이 그 과정을 표현할 수 있습니다. 주의할 것은 $\\textbf{X}$($\\textbf{W}$)의 형상과 역전파로 산출되는 $\\frac{\\partial{L}}{\\partial{\\textbf{X}}}$($\\frac{\\partial{L}}{\\partial{\\textbf{W}}}$)의 형상이 동일하다는 것입니다.\n\n![Backward Propagation of Affine](image/fig-annBack8.png){#fig-annBack8  width=70%}\n\n이러한 어파인 과정을 배치용 계층으로 구성할 때는 $\\textbf{X}$의 형상만을 변형해주면 손쉽게 구현할 수 있습니다. \n\n![Backward Propagation of Affine for Mini-batch](image/fig-annBack9.png){#fig-annBack9  width=70%}\n\n지금까지 설명한 Affine을 코드로 구현하겠습니다. 앞선 예들과 같이 순전파와 역전파 과정을 모두 포함합니다.,\n\n```{python}\nclass Affine:\n  def __init__(self, W, b):\n    self.W = W\n    self.b = b\n    self.x = None\n    self.dw = None\n    self.db = None\n\n  def forward(self, x):\n    self.x = x\n    out = np.dot(x, self.W) + self.b\n    return out\n\n  def backward(self, dout):\n    dx = np.dot(dout. self.W.T)\n    self.dw = np.dot(self.x.T, dout)\n    self.db = np.sum(dout, axis = 0)\n```\n\n### Softmax\n\nSoftmax 계층은 입력값을 정규화 하여 출력하는 계층이라 하였습니다. 이러한 소프트맥스 함수는 손실함수 인 교차 엔트로피 오차도 포함하여 **Softmax-with-loss 계층**으로 마지막 출력층으로 구현됩니다. \n\n![Layer of Softmax-with-loss](image/fig-annBack10.png){#fig-annBack10 width=70%}\n\n::: {.callout-note}\n## 학습과 추론시의 Softmax 함수의 사용여부\n\n-   신경망은 **학습**과 **추론**의 작업수행합니다.\n-   일반적으로 Softmax 계층은 **추론에서는 사용하지 않습니다**.\n    * Classification의 추론문제를 예를 들어보면 산출값 중 최대값 찾는 문제입니다.\n    * 최대값을 찾기 위하여 정규화는 필수적인 요소가 아니고 효율성을 높이기 위해 추론에서는 사용하지 않는 것입니다.\n-   반면 학습에서는 산출된 오차를 역전파를 통하여 매개변수를 업데이트 해야 하므로 정규화된 값이 필요합니다.\n:::\n\n**Forward Propagation**\n\n@fig-annBack11 에서 소프트맥스 계층에 사용되는 함수^[$y_k = \\frac{exp(a_k)}{\\sum_{i=1}^{n}exp(a_i)}$]를 그래프의 순전파 과정은 @fig-annBack11 과 같습니다. 주의할 것은 지수의 합을 $S$로 표기하였다는 점입니다.\n\n![Forward process of Softmax](image/fig-annBack11.png){#fig-annBack11 width=70%}\n\n@fig-annBack12 에서 크로스 엔트로피 오차 계층에 사용되는 함수^[$L=-\\sum_{k}^{}t_k\\log{y_k}$]를 그래프의 순전파 과정은 @fig-annBack12 과 같습니다.\n\n![Forward process of Cross-entropy](image/fig-annBack12.png){#fig-annBack12 width=70%}\n\n**Backward Propagation**\n\n역전파는 순전파의 순서를 바꾸어 크로스 엔트로피 오차의 역전파 과정(@fig-annBack13 )을 먼저 살펴보겠습니다.\n\n![Backward process of Cross-entropy](image/fig-annBack13.png){#fig-annBack13 width=70%}\n\n-   **1단계:** 교차 엔트로피 오차의 역전파 중 초깃값은 @fig-annBack13 에서 가장 오른쪽값인 1($\\frac{\\partial{L}}{\\partial{L}}=1$)입니다.\n-   **2단계:** 첫번째 $\\times$ 노드의 역전파는 상류의 편미분값 `1`과 순전파때 입력값 중 `-1`을 곱하여 `-1`하류로 흘려보냅니다.\n-   **3단계:** $+$ 노드의 역전파는 `-1`을 그대로 하류로 흘려보냅니다.\n-   **4단계:** 두번째 $\\times$ 노드의 역전파는 상류의 편미분값 `-1`과 입력값 중 $t_1$을 곱하여 $-t_1$을 하류로 흘려보냅니다.\n-   **5단계:** $\\log$ 노드의 역전파는 순전파시의 식($y=\\log{x}$)을 편미분한 값($\\frac{\\partial{y}}{\\partial{x}}=\\frac{1}{x}$, 여기서 $x$를 $y_1$로 대입한다.)에 $-t_1$을 곱하여 최종결과를 산출합니다.\n\n다음으로 크로스 엔트로피 오차 계층의 결과($-\\frac{t_1}{y_1}, -\\frac{t_2}{y_2}, -\\frac{t_3}{y_3}$)를 사용하여 소프트맥스의 역전파 과정(@fig-annBack14 )을 살펴보겠습니다. 아래 과정은 크로스 엔트로피 분모쪽의 역전파를 먼저 수행하고 분자쪽의 역잔파를 다음에 수행하며 설명하겠습니다.\n\n![Backward process of Softmax](image/fig-annBack14.png){#fig-annBack14 width=70%}\n\n-   **1단계:** 앞 계층인 크로스 엔트로피 오차 계층의 결과 값의 연전파 값이 역전파의 초깃값($-\\frac{t_1}{y_1}$)에 해당합니다.\n-   **2단계:** 상류에서 입력받은 값과 $\\times$ 노드 중 분모쪽 역전파를 수행해야 하므로 순전파시 분자쪽 입력값($exp(a_1)$)을 곱하여 $-t_1S$ 하류로 흘려보냅니다. [^foot-annBack1]\n\n[^foot-annBack1]: $-t_1S$이 산출되는 과정\n$$\n\\begin{align}\ny_1 = \\frac{\\exp(a_1)}{S} \\quad &\\rightarrow \\quad \\frac{1}{y_1} = \\frac{S}{\\exp(a_1)} \\\\\n-\\frac{t_1}{y_1}\\exp(a_1) &= -t_1\\frac{S}{\\exp(a_1)}\\exp(a_1)=-t_1S\n\\end{align}\n$$\n\n-   **3단계:** $\\div$(/) 노드의 순전파시에 다음 노드로 나누어 흘려 보냈으므로 역전파시에는 나누어진 값($-t_1S, -t_2S, -t_3S$)들을 먼저 합하여야 합니다. 합산된 값($-t_1S+-t_2S+-t_3S = -S(t_1+t_2+t_3)$)에 순전파시 흘려보낸 값($\\frac{1}{S} = S^{-1}$)의 미분값($-S^{-2}=-\\frac{1}{S^2}$)을 곱하여 $\\frac{1}{S}(t_1+t_2+t_3)$^[주의할 것은 $(t_1, t_2, t_3)$은 원-핫 벡터로 이들의 합은 항상 `1`이 됩니다. 따라서, 식을 간단히 하여 하류로 흘려보내는 값은 $\\frac{1}{S}$로 단순화 시킬수 있습니다.]를 하류로 흘려보냅니다.\n-   **4단계:** 분모쪽 $\\div$(/) 노드를 통해 흘러들어온 $\\frac{1}{S}$는 $+$ 노드를 통하여 그대로 $\\frac{1}{S}$을 하류로 흘려보냅니다.\n-   **5단계:** 이제 크로스엔트로피의 분자쪽 방향의 역전파를 살펴보겠습니다. $\\times$ 노드 중 순전파시 분모쪽 입력값($\\frac{1}{S}$)를 상류에서 역전파를 위해 입력받은 값($-\\frac{t_1}{y_1}$)과 곱하여 $-\\frac{t_1}{y_1}\\frac{1}{S}$^[이식에서 $y_1=\\frac{exp(a_1)}{S}$를 활용하여 $-\\frac{t_1}{exp(a_1)}$로 단순화 킬수 있습니다.]를 하류로 흘려보냅니다.\n-   **6단계:** `EXP`노드는 앞서 부모쪽 $\\div$(/) 노드처럼 순전파시 $exp(a_1)$ 을 다음노드로 나누어 흘려 보냈으므로 역전파시에는 나누어진 값(부모쪽:$\\frac{1}{S}$, 분자쪽:$-\\frac{t_1}{exp(a_1)}$)들을 먼저 합하여야 합니다. 이렇게 합한 값에 순전파시 흘려보낸 값($y=\\exp(a_1)$)의 미분값($\\frac{\\partial{y}}{\\partial{x}}=\\exp(x)$)을 곱하여 최종적으로 $\\frac{\\exp(a_1)}{S}-t_1$[^foot_annBack2] 소프트맥스 계층의 역전파 값을 산출합니다.\n\n[^foot_annBack2]: $(\\frac{1}{S}-\\frac{t_1}{\\exp(a_1)\\exp(a_1)})$\n","srcMarkdownNoYaml":""},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","html-math-method":"katex","reference-location":"margin","output-file":"ANN-backward.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.4.551","bibliography":["references.bib"],"grid":{"sidebar-width":"300px","body-width":"900px","margin-width":"300px","gutter-width":"1.5rem"},"mermaid":{"theme":"neutral"},"theme":"lumen","fig-cap-location":"bottom","tbl-cap-location":"bottom","citation-location":"margin"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}