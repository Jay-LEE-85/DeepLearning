{"title":"Lab: Implementing ANN with MINIST dataset","markdown":{"headingText":"Lab: Implementing ANN with MINIST dataset","containsRefs":false,"markdown":"---\n\nANN에서 필요한 활성화함수, 손실함수, 미니배치 및 경사하강법(기울기)에 학습내용을 바탕으로 신경망을 실제로 구현해보도록 하겠습니다. 참고로 미니배치를 이용하여 확률적으로 데이터를 무작위로 추출한 경우를 **확률적 경사하강법**(SGD: stochastic gradient descent)라고 합니다.\n\n구현할 학습알고리즘은 MNIST 데이터 셋을 활용하여 손글씨 숫자를 인식하는 ANN으로 먼저 ANN 클래스를 구현하고 이후 미니배치를 통한 학습 알고리즘을 구현하도록 하겠습니다.^[TwoLayerNet은 스탠퍼드의 CS231n  수업에서 제공한 코드를 참고]\n\n## STEP1: Two Layer Net\n\n우리는 지금까지 학습한 내용에 기초하여 아래와 같이 순전파(forward propagation) 과정을 **Two Layer Net**이라는 클래스로 구현해 보겠습니다. \n\n``` {python}\n#| eval: False\nimport sys, os\nsys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\nfrom common.functions import *\nfrom common.gradient import numerical_gradient\n\n\nclass TwoLayerNet:\n\n    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01): # <1>\n        # 가중치 초기화\n        self.params = {}\n        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n        self.params['b1'] = np.zeros(hidden_size)\n        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n        self.params['b2'] = np.zeros(output_size)\n\n    def predict(self, x): # <2>\n        W1, W2 = self.params['W1'], self.params['W2']\n        b1, b2 = self.params['b1'], self.params['b2']\n    \n        a1 = np.dot(x, W1) + b1\n        z1 = sigmoid(a1)\n        a2 = np.dot(z1, W2) + b2\n        y = softmax(a2)\n        \n        return y\n        \n    # x : 입력 데이터, t : 정답 레이블\n    def loss(self, x, t): # <3>\n        y = self.predict(x)\n        \n        return cross_entropy_error(y, t)\n    \n    def accuracy(self, x, t): # <4>\n        y = self.predict(x)\n        y = np.argmax(y, axis=1)\n        t = np.argmax(t, axis=1)\n        \n        accuracy = np.sum(y == t) / float(x.shape[0])\n        return accuracy\n        \n    # x : 입력 데이터, t : 정답 레이블\n    def numerical_gradient(self, x, t): # <5>\n        loss_W = lambda W: self.loss(x, t)\n        \n        grads = {}\n        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n        \n        return grads\n        \n    def gradient(self, x, t): # <6>\n        W1, W2 = self.params['W1'], self.params['W2']\n        b1, b2 = self.params['b1'], self.params['b2']\n        grads = {}\n        \n        batch_num = x.shape[0]\n        \n        # forward\n        a1 = np.dot(x, W1) + b1\n        z1 = sigmoid(a1)\n        a2 = np.dot(z1, W2) + b2\n        y = softmax(a2)\n        \n        # backward\n        dy = (y - t) / batch_num\n        grads['W2'] = np.dot(z1.T, dy)\n        grads['b2'] = np.sum(dy, axis=0)\n        \n        da1 = np.dot(dy, W2.T)\n        dz1 = sigmoid_grad(a1) * da1\n        grads['W1'] = np.dot(x.T, dz1)\n        grads['b1'] = np.sum(dz1, axis=0)\n\n        return grads\n```\n1. `__init__` 클래스의 초기화 수행 (입력층, 은닉층, 출력층 각각의 뉴런수 지정)\n2. `predict` 추론을 수행 (x 이미지 데이터)\n3. `loss` 손실함수 값 산출 (x 이미지 데이터, t 정답레이블)\n4. `accuracy` 신경망의 정확도 산출\n5. `numerical_gradient` 매개변수(가중치)의 기울기 산출\n6. `gradient` 매개변수(가중치)의 기울기 산출(오차 역전파과정은 다음장에서 설명)\n\n위의 클래스에서 `gradient` 메서드의 경우 순전파와 역전파를 모두 사용하고, 순전파시 활성화 함수로 `sigmoid`를 출력값의 활성화 함수로 분류 문제해결을 위해 `softmax`를 사용하였습니다.(제사한 사항은 활성화 함수를 참고 @sec-annFor) \n\n추가적으로 우리가 사용할 데이터의 형상에 주의하여 클래스의 초기화를 수행해야 합니다. **MNIST** 데이터 셋의 개별 입력값은 $28 \\times 28$ 픽셀을 Flatten하게하여 784개의 입력값의 형상을 설정해야 합니다.\n\n또한, 우리가 수행하는 분류문제에서 출력값은 분류하고자 하는 카테고리의 갯수 여기서 **0~9**까지 10개의 숫자를 분류해야 함을 고려하여 출력값의 형상은 10개로 설정해야 합니다.\n\n## STEP2: Mini-Batch\n\n모든 데이터를 갖고 학습하는 것은 효율적이지 않다고 하였습니다. 따라서, 무작위로 훈련 데이터 중 일부를 추출하여 훈련을 진행하는 미니배치 방법을 통하여 훈련의 효율성을 높일 수 있습니다.\n\n미니배치를 활용하여 확률적 경사하강법을 구현하면 아래와 같습니다.\n\n``` {python}\n#| eval: False\nimport sys, os\nsys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom dataset.mnist import load_mnist\n\n# 데이터 읽기\n(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n\nnetwork = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n\n# 하이퍼파라미터\niters_num = 10000  # 반복 횟수를 적절히 설정한다.\ntrain_size = x_train.shape[0]\nbatch_size = 100   # 미니배치 크기\nlearning_rate = 0.1\n\ntrain_loss_list = []\n\nfor i in range(iters_num):\n    # 미니배치 획득\n    batch_mask = np.random.choice(train_size, batch_size)\n    x_batch = x_train[batch_mask]\n    t_batch = t_train[batch_mask]\n    \n    # 기울기 계산\n    #grad = network.numerical_gradient(x_batch, t_batch)\n    grad = network.gradient(x_batch, t_batch)\n    \n    # 매개변수 갱신\n    for key in ('W1', 'b1', 'W2', 'b2'):\n        network.params[key] -= learning_rate * grad[key]\n    \n    # 학습 경과 기록\n    loss = network.loss(x_batch, t_batch)\n    train_loss_list.append(loss)\n```\n\n## STEP3: Evaluating with test data\n\n이제 우리가 구현한 클래스의 성능을 평가해 보도록 하겠습니다. 훈련데이터를 미니배치로 나누어 훈련을 진행하였는데 과연 다른 새로운 데이터셋에서도 동일한 성능을 발휘할지 알아야 합니다.\n\n만약, 훈련 데이터에만 적응한 ANN이라면 새로운 데이터에서는 적절한 성능을 발휘하지 못할 가능성이 있기 때문입니다. 이를 **오버피팅**(overfitting)^[오버피팅은 훈련데이터에 대한 정확도는 높으나 신규 데이터에는 적절한 성능을 발휘하지 못하는 문제로 이러한 문제를 해결하기 위하여 조기종료(early stopping), 가중치 감소, 드롭아웃(drop-out)등의 기법이 사용됩니다.]이라 합니다.\n\n``` {python}\n#| eval: False\nimport sys, os\nsys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom dataset.mnist import load_mnist\n\n# 데이터 읽기\n(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n\nnetwork = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n\n# 하이퍼파라미터\niters_num = 10000  # 반복 횟수를 적절히 설정한다.\ntrain_size = x_train.shape[0]\nbatch_size = 100   # 미니배치 크기\nlearning_rate = 0.1\n\ntrain_loss_list = []\ntrain_acc_list = [] # <1>\ntest_acc_list = [] # <2>\n\niter_per_epoch = max(train_size / batch_size, 1) # <3>\n\nfor i in range(iters_num):\n    # 미니배치 획득\n    batch_mask = np.random.choice(train_size, batch_size)\n    x_batch = x_train[batch_mask]\n    t_batch = t_train[batch_mask]\n    \n    # 기울기 계산\n    #grad = network.numerical_gradient(x_batch, t_batch)\n    grad = network.gradient(x_batch, t_batch)\n    \n    # 매개변수 갱신\n    for key in ('W1', 'b1', 'W2', 'b2'):\n        network.params[key] -= learning_rate * grad[key]\n    \n    # 학습 경과 기록\n    loss = network.loss(x_batch, t_batch)\n    train_loss_list.append(loss)\n    \n    # 정확도 계산\n    if i % iter_per_epoch == 0: # <4>\n        train_acc = network.accuracy(x_train, t_train)\n        test_acc = network.accuracy(x_test, t_test)\n        train_acc_list.append(train_acc)\n        test_acc_list.append(test_acc)\n        print(\"train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))\n\n# 그래프 그리기\nmarkers = {'train': 'o', 'test': 's'}\nx = np.arange(len(train_acc_list))\nplt.plot(x, train_acc_list, label='train acc')\nplt.plot(x, test_acc_list, label='test acc', linestyle='--')\nplt.xlabel(\"epochs\")\nplt.ylabel(\"accuracy\")\nplt.ylim(0, 1.0)\nplt.legend(loc='lower right')\nplt.show()\n```\n1. `train_acc_list` 학습 데이터에 대한 정확도를 저장하는 튜플\n2. `test_acc_list` 시험 데이터에 대한 정확도를 저장하는 튜플\n3. `iter_per_epoch` 1 epoch당 반복하는 횟수\n4. `if i % iter_per_epoch == 0:` 1 epoch당 정확도 계산\n\n![Accuracy trends for training and test data](image/annTLN.png){#fig-annTLN}\n\n위의 그림에서 보듯이 훈련 데이터(실선)와 시험 데이터(점선)의 정확도가 epoch인 진행될 수록 같은 수준으로 좋아지고 있습니다. 이는 오버피팅없이 적절히 학습이 이루어 졌다고 평가할 수 있습니다.\n\n\n\n","srcMarkdownNoYaml":""},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","html-math-method":"katex","reference-location":"margin","output-file":"ANN-lab_MNIST.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.4.551","bibliography":["references.bib"],"grid":{"sidebar-width":"300px","body-width":"900px","margin-width":"300px","gutter-width":"1.5rem"},"mermaid":{"theme":"neutral"},"theme":"lumen","fig-cap-location":"bottom","tbl-cap-location":"bottom","citation-location":"margin"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}