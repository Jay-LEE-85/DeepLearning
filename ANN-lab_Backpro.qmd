# Lab: Implementing Backpropagation to ANN 
---

지금까지 살펴본 내용을 종합하여 계층들을 조합한 신경망을 구축해 보도록 하겠습니다.  구체적인 구현에 앞서 전체적인 이해를 위해 전체과정을 다시 살펴볼 필요가 있습니다.

-   **Pre requisite**
    + 신경망에는 적응 가능한 가중치와 편향이 있고, 이 가중치와 편향을 훈련 데이터에 적응하도록 조정하는 과정을 **학습**이라 한다.
-   **1단계 - 미니배치**
    + 훈련 데이터 중 일부를 무작위로 추출하고, 이러한 미니배치를 활용하여 미니배치의 손실함수 값을 줄이도록 하는 것을 목표로 한다.
-   **2단계 - 기울기 산출**
    + 미니배치의 손실함수 값을 줄이기 위해 각 가중치 매개변수의 기울기루 구하고, 기울기는 손실함수의 값을 가장작게하는 방향으로 제시한다.
-   **3단계 - 매개변수 갱신**
    + 가중치 매개변수를 기울기의 반대방향(책에는 기울기 방향으로 되어 있는데?)으로 아주 조금 갱신한다.
-   **4단계 - 반복(Iteration)**
    + 1~3단계를 반복합니다.

오차 역전파는 2단계인 기울기 산출에 해당하고 기울기는 수치미분을 활용함을 다시 떠올려보고, 이를 효율적으로 수행하기 위하여 미니배치를 활용한다는 것까지 생각하며 다음의 구현사례를 살펴보도록 합시다.

## STEP1: Two Layer Net

```{python}
import sys, os
sys.path.append(os.pardir)
import numpy as np

from common.layers import *
from common.gradient import numerical_gradient
from collections import OrderDict

class TwoLayerNet:

  def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):    
    # 가중치 초기화
    self.params = {}
    self.params['W1'] = weight_init_std * np,random,randn(input_size, hidden_size)
    self.params['b1'] = np.zeros(hidden_size)
    self.params['W2'] = weight_init_std * np,random,randn(input_size, hidden_size)
    self.params['b2'] = np.zeros(output_size)
    # 계층 생성
    self.layers = OrderDict
    self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])
    self.layers['Relu1'] = Relu()
    self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])
    self.lastlayers = SoftmaxWithLoss()

  def predict(self, x):
    for layer in self.layers.values():
      x = layer.forward(x)
      return x

  # x : 입력데이터, t : 정답레이블
  def loss(self, x, t):
    y = self.predict(x)
    return self.lastLyer.forward(y, t)
```

## STEP2: Gradient Check

```{python}
```

## STEP3: Implementing Backpropagation to ANN

```{python}
```