# Learning process of ANN
---
``` {python}
#| echo: false
import numpy as np
import matplotlib.pyplot as plt
```

인공싱경망은 학습과 추론 과정으로 구분할 수 있습니다. 인공신경망을 만든다(?)는 관점은 학습을 의미하며 이는 신경망의 출력값과 실제값의 차이를 줄이는 방향으로 신경망을 이루는 가중치(매개변수)를 찾는 과정이라고 단순히 말할 수 있습니다.

여기서 중요한 것은 인간이 매개변수를 설정하지 않고 기계가 스스로 찾는 다는 점에서 매우 의미가 있습니다. 퍼셉트론에서 진리표의 매개변수는 3개에 불과하여 수작업이 가능했지만 매개변수가 1000개라고 하면 수작업이 불가할 것 입니다. 이 과정을 기계가 스스로 학습하여 해결한다니 매우 놀라운 일입니다.

그리고 학습을 통해 매개변수를 기계가 찾는 다는 것은 주어진 데이터의 특성(Features)를 추출하여 특정 패턴을 찾는 것입니다. 물론 사람의 개입없이 말입니다.

우리가 집중할 것은 이러환 과정이 어떻게 구현되는가 입니다. 그래서 다음의 주제에 대하여 집중하여 살펴보도록 하겠습니다.

1.  출력값과 실제값을 비교하여 정확도를 측정하는 **손실함수**(loss function)
2.  손실값을 활용한 가중치 갱신을 위한 **경사하강법**(gradient descent)
    -   pre-requisite: 수치미분을 통한 기울기 산출
    -   applicatoin: 학습률(learing rate $\eta$: $\omega = \omega - \eta\frac{\partial{f}}{\partial{\omega}}$)을 이용한 매개변수 갱신

``` {mermaid}
%%| label: fig-annLearn
%%| fig-cap: "Forward and Backward propagation"

flowchart LR
  input[input] --"forward" ---> loss[loss]
  loss[loss] --"backward" ---> input[input]
```

손실함수는 추론(?)에 해당하는 순전파 과정에서 그 정확도를 산출하기 위한 내용이고, 경사하강법은 산출된 Loss를 입력까지의 역전파하여 매개변수인 가중치를 업데이트 하는 과정에 대한 내용으로 단순하게 이해할 수 있고 자세한 사항을 아래에 섦영하도로고 하겠습니다.

## Loss Function

손실함수는 ANN에 입력되어 산출되는 값과 실제 값을 비교하여 **정확도**를 측정하는 하나의 **지표**라고 할 수 있다. ANN은 이러한 지표를 기준으로 최적의 가중치를 탐색하여 하나의 모델을 구성하게 됩니다.

손실함수로는 일반적으로 **오차제곱합**(SSE, sum of squares for error)과 **교차 엔트로피 오차**(CEE, cross entropy error)를 사용합니다.

### SSE(sum of squares for error)

오차제곱합(@eq-annSSE )은 신경말의 출력값($y_k$)과 실제값($t_k$) 사이의 차이인 오차를 계산하고 이 오차를 제곱하여 모두 더한 값을 말하며 이 값이 작아질 수록 모델이 더 좋은 예측능력을 보유한다고 판단합니다.

$$
E = \frac{1}{2}\sum^{}_{k}(y_k - t_k)^2
$$ {#eq-annSSE}

오차제곱합의 작동원리를 이해하기 위하여 임의로 **원-핫 인코딩**된 레이블($t_k$)과 임의로 **소프트맥수 합수**의 출력값($y_k$)을 생성하여 코드로 구현해 보겠습니다.^[CEE에서도 동일한 예제를 사용 예정]

``` {python}
# 정답 레이블은 2
t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]

def sum_squres_error(y, t):
  return 0.5 * np.sum((y-t)**2)

# 예1: 2일 확률이 제일 높음(60%)
y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]
l1 = sum_squres_error(np.array(y), np.array(t))

# 예2: 7일 확률이 제일 높음(60%)
y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]
l2 = sum_squres_error(np.array(y), np.array(t))

print("1번예제: %3f, 2번예제:%3f" % (l1, l2))
```

정답 레이블인 `2`를 높은 확률로 산출한 1번 예제의 손실값이 0.09로 0에 근접하고, `7`를 높은 확률로 산출한 2번 예제의 손실값이 0.59로 0에 멀게 산출됨을 확인 할 수 있습니다.

### CEE(cross entropy error)

교차 엔트로피(@eq-annCEE )는 신경망의 출력값($y_k$)이 소프트 맥스 함수를 거쳐 확률로 [0.0, 1.0]의 값을 갖는 다는 점을 고려하여 확률값이 높을 수록 0에 수렴하는 손실값을 산출합니다.

$$
E = -\sum^{}_{k}t_k\log{y_k}
$$ {#eq-annCEE}

``` {python}
#| echo: False
#| label: fig-annPlotLog
#| fig-cap: Graph of y=lox(x)

# x 값의 범위를 설정합니다.
x = np.linspace(0.001, 1.0, 400)

# 자연로그 값을 계산합니다.
y = np.log(x)

# 그래프를 그립니다.
plt.figure(figsize=(8, 6))
plt.plot(x, y, label='y = log(x)')
plt.axhline(0, color='black',linewidth=0.5)
plt.axvline(0, color='black',linewidth=0.5)
plt.grid(color = 'gray', linestyle = '--', linewidth = 0.5)
plt.xlabel('x')
plt.ylabel('y')
plt.legend()
plt.show()
```



## Gradient Descent

