# Learning process
---
``` {python}
#| echo: false
import numpy as np
import matplotlib.pyplot as plt
```

인공싱경망은 학습과 추론 과정으로 구분할 수 있습니다. 인공신경망을 만든다(?)는 관점은 학습을 의미하며 이는 신경망의 출력값과 실제값의 차이를 줄이는 방향으로 신경망을 이루는 가중치(매개변수)를 찾는 과정이라고 단순히 말할 수 있습니다.

여기서 중요한 것은 인간이 매개변수를 설정하지 않고 기계가 스스로 찾는 다는 점에서 매우 의미가 있습니다. 

퍼셉트론에서 진리표의 매개변수는 3개에 불과하여 수작업이 가능했지만 매개변수가 1000개라고 하면 수작업이 불가할 것 입니다. 이 과정을 기계가 스스로 학습하여 해결한다니 매우 놀라운 일입니다.

그리고 학습을 통해 매개변수를 기계가 찾는 다는 것은 주어진 데이터의 특성(Features)를 추출하여 특정 패턴을 찾는 것입니다. 물론 사람의 개입없이 말입니다.

우리가 집중할 것은 이러환 과정이 어떻게 구현되는가 입니다. 그래서 다음의 주제에 대하여 집중하여 살펴보도록 하겠습니다.

1.  출력값과 실제값을 비교하여 정확도를 측정하는 **손실함수**(loss function)
2.  손실값을 활용한 가중치 갱신을 위한 **경사하강법**(gradient descent)
    -   pre-requisite: 수치미분을 통한 기울기 산출
    -   applicatoin: 학습률(learing rate $\eta$: $\omega = \omega - \eta\frac{\partial{f}}{\partial{\omega}}$)을 이용한 매개변수 갱신

``` {mermaid}
%%| label: fig-annLearn
%%| fig-cap: "Forward and Backward propagation"

flowchart LR
  input[input] --"forward" ---> loss[loss]
  loss[loss] --"backward" ---> input[input]
```

손실함수는 추론(?)에 해당하는 순전파 과정에서 그 정확도를 산출하기 위한 내용이고, 경사하강법은 산출된 Loss를 입력까지의 역전파하여 매개변수인 가중치를 업데이트 하는 과정에 대한 내용으로 단순하게 이해할 수 있고 자세한 사항을 아래에 섦영하도로고 하겠습니다.

## Loss Function

손실함수는 ANN에 입력되어 산출되는 값과 실제 값을 비교하여 **정확도**를 측정하는 하나의 **지표**라고 할 수 있다. ANN은 이러한 지표를 기준으로 최적의 가중치를 탐색하여 하나의 모델을 구성하게 됩니다.

손실함수로는 일반적으로 **오차제곱합**(SSE, sum of squares for error)과 **교차 엔트로피 오차**(CEE, cross entropy error)를 사용합니다.

### SSE(sum of squares for error)

오차제곱합(@eq-annSSE )은 신경말의 출력값($y_k$)과 실제값($t_k$) 사이의 차이인 오차를 계산하고 이 오차를 제곱하여 모두 더한 값을 말하며 이 값이 작아질 수록 모델이 더 좋은 예측능력을 보유한다고 판단합니다.

$$
E = \frac{1}{2}\sum^{}_{k}(y_k - t_k)^2
$$ {#eq-annSSE}

오차제곱합의 작동원리를 이해하기 위하여 임의로 **원-핫 인코딩**된 레이블($t_k$)과 임의로 **소프트맥수 합수**의 출력값($y_k$)을 생성하여 코드로 구현해 보겠습니다.^[CEE에서도 동일한 예제를 사용 예정]

``` {python}
# 정답 레이블은 2
t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]

def sum_squres_error(y, t):
  return 0.5 * np.sum((y-t)**2)

# 예1: 2일 확률이 제일 높음(60%)
y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]
l1 = sum_squres_error(np.array(y), np.array(t))

# 예2: 7일 확률이 제일 높음(60%)
y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]
l2 = sum_squres_error(np.array(y), np.array(t))

print("1번예제: %3f, 2번예제:%3f" % (l1, l2))
```

정답 레이블인 `2`를 높은 확률로 산출한 1번 예제의 손실값이 0.09로 0에 근접하고, `7`를 높은 확률로 산출한 2번 예제의 손실값이 0.59로 0에 멀게 산출됨을 확인 할 수 있습니다.

### CEE(cross entropy error)

교차 엔트로피(@eq-annCEE )는 신경망의 출력값($y_k$)이 소프트 맥스 함수를 거쳐 확률로 [0.0, 1.0]의 값을 갖는 다는 점을 고려하여 확률값이 높을 수록 0에 수렴하는 손실값을 산출합니다.

$$
E = -\sum^{}_{k}t_k\log{y_k}
$$ {#eq-annCEE}

``` {python}
#| echo: False
#| label: fig-annPlotLog
#| fig-cap: Graph of y=lox(x)

# x 값의 범위를 설정합니다.
x = np.linspace(0.001, 1.0, 400)

# 자연로그 값을 계산합니다.
y = np.log(x)

# 그래프를 그립니다.
plt.plot(x, y, label='y = log(x)')
plt.axhline(0, color='black',linewidth=0.5)
plt.axvline(0, color='black',linewidth=0.5)
plt.grid(color = 'gray', linestyle = '--', linewidth = 0.5)
plt.xlabel('x')
plt.ylabel('y')
plt.legend()
plt.show()
```

위의 그래프(@fig-annPlotLog )에서 보듯 $x$가 1일때 $y$는 0이 되고 $x$가 0에 근접시 $y$는 점점 작아집니다. 교차 엔트로피는 여기에 음(`-`)의 부호를 붙여 활률값인 `x`가 작아질수록 손실값인 `y`가 크게 나오도록 하였습니다.

위의 식(@eq-annCEE )을 코드로 구현해 보겠습니다.

``` {python}
# 정답 레이블은 2
t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]

def corss_entropy_error(y, t):
  delta = 1e-7 # <1>
  return -np.sum(t * np.log(y + delta)) # <1>

# 예1: 2일 확률이 제일 높음(60%)
y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]
l1 = corss_entropy_error(np.array(y), np.array(t))

# 예2: 7일 확률이 제일 높음(60%)
y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]
l2 = corss_entropy_error(np.array(y), np.array(t))

print("1번예제: %3f, 2번예제:%3f" % (l1, l2))
```
1. `delta`를 설정한 이유는 `y`가 `0`이면 `np,log`함수는 무한대인 `inf`값을 출력하는데 이를 방지하기 위함

1번 예제의 손실값이 `0.51`로 2번 예제의 손실값보다 낮은 값을 출력하여 오차제곱합의 결과와 일치함을 확인할 수 있습니다.

### CEE with Mini-batch

손실함수는 주어진 데이터에 대한 모든 손실값의 합을 모델의 평가지료로 산출합니다. 가령 훈련 데이터가 100개이 60,000개 인경우 60,000개의 손실값을 산출해야 합니다.

이렇게 훈련데이터 모두에 대한 손실값을 산출하는 경우 데이터가 증가할 수록 산출시간이 오래 걸리는 문제가 있습니다. 그렇다면 이를 보다 효율적으로 할 수 있는 방법은 무엇이 있을까요?

일부 데이터를 추려 **근사치**를 계산하는 방법을 생각할 수 있습니다. 이러한 방법을 **미니배치**(mini-batch)라고 하며 훈련데이터 전체에서 임의로 특정 데이터를 뽑아 학습하는 방법입니다.

우리는 이러한 **미니배치 학습**을 위해 몇개를 임의 추출할지(`batch-size`)와 이러한 과정을 몇번 수행할 것인지(`number of iteration`)에 대한 고민을 해야 합니다, 그 이유는 배치는 전체데이터의 일부만을 대상으로 하기 때문입니다.^[본 문제는 hyper parameter의 설정에 대한 내용으로 overfitting을 방지하려는 문제와 관련이 있습니다.]

``` {python}
import sys, os
sys,path.append(os.pardir)
from dataset.mnist import load_mnist

# MNIST 데이터셋 로드
(x_train, t_train), (x_test, t_test) = /
  load_mnist(normalise = True, one_hot_label = True)

# Mini-batch 설정
train_size = x_train.shape[0]
batch_size = 10
batch_mask = np.random.choice(train_size, batch_size) # <1>

# Batch용 데이터 설정
x_batch = x_train[batch_mask] # <2>
t_batch = t_train[batch_mask] # <2>
```
1. `np.randon.choice`는 전체 N개의 데이터(`train_size`)에서 임의로 몇개(`batch_size`)를 추출할 것인지 결정
2. `batch_mask`는 임의 뽑힌 데이터의 인덱스 값

위에서 우리는 학습을 효율적으로 실시하기 위하여 미니배치를 설정하였습니다. 그렇다면 미니배치로 손실을 어떻게 구해야 할까고민입니다. 앞서 구현한 교차 엔트로피 오차를 아래의 코드와 같이 일부만 수정하면 간단히 해결할 수 있습니다.

``` {python}
def corss_entropy_error(y, t):
  if y.ndim == 1:
    t = t.reshape(1, t.size)
    y = y.reshape(1, y.size)
  
  batch_size = y.shape[0]
  return -np.sum(t * np.log(y + 1e-7)) / batch_size                            # <1>
  return -np.sum(t * np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size  # <2>
```
1. 정답 레이블이 **원-핫 인코딩** 형상인 경우
2. 정답 레이블이 **숫자 레이블**인 경우



## Gradient Descent

