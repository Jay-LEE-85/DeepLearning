# Artificial Neural Networks

**인공신경망(ANN, Artificial Neural Networks)**은 퍼셉트론과 유사한 메커니즘을 갖고 있습니다. @fig-egANN 에서 가장 왼쪽이 **입력층**(Input), 중간이 **은닉층**(Hidden), 가장 오른쪽이 **출력층**(Output)으로 구성되어 있습니다.

``` {mermaid}
%%| label: fig-egANN
%%| fig-cap: "Example of ANN"
graph LR
  subgraph Input
      direction LR
      x1((x1)) & x2((x2)) 
  end
  
  subgraph Hidden
      direction LR
      h1((h1)) & h2((h2)) & h3((h3))
  end 

  subgraph Output
      direction LR
      y1((y1)) & y2((y2))
  end

    x1((x1)) & x2((x2))  ---> h1((h1)) & h2((h2)) & h3((h3))
    h1((h1)) & h2((h2)) & h3((h3)) ---> y1((y1)) & y2((y2))
```

다만, 퍼셉트론과 다른 점이 있따면, 신호를 전달 받는 과정에서 편향에 해당하는 `b`가 명시적으로 존재하여 이 또한 신호로 처리한다는 부분입니다.

::: {#fig-percepronVSann layout-ncol=2}

::: {#first-column}
``` {mermaid}
%%| label: fig-percepronVSann_1
%%| fig-cap: "Perceptron"
flowchart LR
  x1((x1)) --w1---> y((y))
  x2((x2)) --w2---> y
```
:::

::: {#second-column}
``` {mermaid}
%%| label: fig-percepronVSann_2
%%| fig-cap: "ANN"
flowchart LR
  x0((1)):::bias --b---> y((y))
  x1((x1))  --w1---> y((y))
  x2((x2))  --w2---> y
  classDef bias fill:#f96
```
:::

퍼셉트론과 ANN의 비교
:::

@fig-percepronVSann 에는 잘 나타나 있지 않지만 **Perceptron**의 경우 입력신호를 받아 `y`를 바로 출력하지만, **ANN**의 경우 입력신호와 가중치를 곱하여 총합을 산출하는 함수와 이 산출값을 이용하여 조건 분기의 동작(0을 넘으면 1을 출력하고 그렇지 않으면 0을 출력)을 나타내는 함수로 구성되어 있으며 이를 구현한 산식은 @eq-ann 과 같이 나타낼 수 있다.

$$
\begin{align}
&y = h(b + w_{1}x_{1} + w_{2}x_{2}) \\ \\
&h(x) = 
  \begin{cases}
  0 & (x \leq 0) \\
  1 & (x > 1)
  \end{cases}
\end{align}
$$ {#eq-ann}

## Activation Function

앞서 살펴본 $h(x)$ 라는 함수가 바로 **활성화 함수**(Activation function)이다. 이는 입력신호의 총합(Weighted Sum)이 다음 뉴런이 활성화 정도를 결정하는 함수로 이해할 수 있습니다.

이러한 활성화 함수가 포함된 ANN의 기본적인 모형은 다음과 같이 수정할 수 있다. `a`는 입력신호의 총합이를 받아 `h()`함수를 거쳐 `y`를 출력합니다.

``` {mermaid}
%%| label: fig-egANN
%%| fig-cap: "Processing of the activation function"

graph LR
  subgraph h ["h()"]
    direction LR
    a((a)) & y((y))
  end

  x0((1)):::bias --b---> a
  x1((x1))  --w1---> a
  x2((x2))  --w2---> a
  a --> y
  classDef bias fill:#f96
```

퍼셉트론에서는 활성화 함수로 **계단함수**(Sign function)를 사용하였느나, ANN에서는 활성화 함수로 미분가능한 함수들을 사용합니다. 다음은 ANN에서 사용하는 활성화 함수에 대하여 소개하겠습니다.

### Sigmoid Function

### ReLU Function

### Softmax Function