# Batch Normalization
---

이전 장에서 가중치 초깃값을 적절히 설정하명 각 층의 활성함수값 분포가 균일화되어 학습이 원활하게 수행됨을 확인하였습니다. 

가중치 초깃값 없이 각 층의 활성함수값 분포를 강제로 균일하게 퍼지도록 할 수 없을까요? 이러한 물음에서 시작한 논의의 결과가 바로 ***배치정규화***(Batch Normalization)입니다.

## Algorithm of Batch Normalization

배치정규화는 학습의 효율성과 정확성을 증가시키는 등 강점은 명확합니다.

-   학습을 빨리 진행할수 있다.(학습속도 개선)
-   초깃값에 크게 의존하지 않는다(골치아픈 초깃값 선택 장애 회피)
-   오버피팅을 억제한다(드롭아웃 등의 필요성 감소)

그렇다면 배치정규화는 무엇일까요? 배치정규화는 이름에서 알수 있듯이 미니배치를 단위로 정규화를 진행하는 것으로 데이터 분포가 평균 0, 분산이 1이 되도록 정규화 하는 방법으로 그 수식은 아래와 같습니다.

$$
\begin{align}
\mu_{B} &\leftarrow \frac{1}{m}\sum^{m}_{i=1}x_i \\
\sigma^2_B &\leftarrow \frac{1}{m}\sum^{m}_{i=1}(x_i-\mu_B)^2 \\
\hat{x}_i &\leftarrow \frac{x_i-\mu_B}{\sqrt{\sigma^2_B+\epsilon}}
\end{align}
$$

미니배치($B=\{x_1, x_2, \cdots, x_m\}$)에 m개의 입력 데이터가 존재할때 해당 미니배치의 평균과 분산을 구하여 입력 데이터를 평균이 0, 분산이 1이되도록 정규화를 진행합니다.^[$\epsilon$은 아주 작은값을 의미하며 0으로 나누게 되는 경우를 방지하기 위합입니다.]

이렇게 정규화한 데이터를 활성화함수 앞 또는 뒤에 삽입함으로써 활성화 함수값의 분포를 고르게 만들 수 있습니다.

또 하나 주목할 만한 것은 정규화 계층마다 고유한 확대(scale) 및 이동(shift) 변환을 수행할 수 있다는 것이고 이러한 확대 및 변환에 관한 수식은 아래와 같습니다.

$$
y_i \leftarrow \gamma\hat{x}_i+\beta
$$

여기서 $\gamma$는 확대를 담당하고 1부터 시작합니다. 그리고 $\beta$는 이동을 담당하고 0부터 시작합니다.

이 알고리즘은 순전파때 적용되고 아래와 같이 계산그래프(@fig-techBN1 )로 표현이 가능합니다.

![Graph of process for Batch Normalization](image/fig-techBN1.png){#fig-techBN1}

이 알고리즘의 역전파에 대한 계산그래프는 추후 정리하여 업데이트 하도록 하겠습니다.

## Effects of Batch Normalization

배치 졍규화를 실시하면 사용하지 않을 때보다 학습 진도(?)가 더 빨라진다는 것을 확인할 수 있습니다.

![Effects of Batch Normalization](image/fig-techBN2.png){#fig-techBN2}

배치 정규화를 사용한 경우에 빨라지는 학습 진도(?)는 가중치 초깃값의 표준편차를 무엇으로 하느냐에 크게 영향을 받지않고 있음을  아래의 그림을 통해 확인할 수 있습니다.

![Effects of Batch Normalization](image/fig-techBN3.png){#fig-techBN3}