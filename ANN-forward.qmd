# Forward Propagation
---
``` {python}
#| echo: false
import numpy as np
import matplotlib.pyplot as plt
from func_set import *
```
신경망에서의 데이터의 기본적인 입력층에서 출력까지의 데이터의 흐름(처리 과정)에 대하여 살펴보겠습니다.

아래에 예시는 입력층(0층)의 노드 2개, 은닉층(1층)의 노드 3개, 은닉층(2층)의 노드 2개를 거쳐 2개의 노드를 갖는 출력층(3층)으로 구성된 신경망 입니다.

![signaling](/image/ann.png){#fig-forANN0 width=70%}

@fig-forANN0 에서 $a^{(1)}_1$의 경우 입력갑의 경우 가중치와 곱한 값을 합산한 Weighted Sum 값과 편향(bias)더하여 아래와 같이 산출할 수 있고 이를 Vector form으로 간소화 할 수 있습니다.

$$ 
\begin{align}
a^{(1)}_{1} &= w^{(1)}_{11}+x^{}_{1}+w^{(1)}_{12}x_2+b^{(1)}_{1} \\
\textbf{A}^{(1)} &= \textbf{X}\textbf{W}^{(1)} + \textbf{B}^{(1)}
\end{align}
$$ {#eq-forANN}

::: {.column-margin}
$$
\begin{align}
\textbf{A}^{(1)} &= (a^{(1)}_{1}\, a^{(1)}_{2}\, a^{(1)}_{3}) \\
\textbf{X} &= (x_1\, x_2) \\
\textbf{B}^{(1)} &= (b^{(1)}_{1}, b^{(1)}_{2}, b^{(1)}_{3}) \\
\textbf{W}^{(1)} &= \begin{pmatrix}
    w^{(1)}_{11} & w^{(1)}_{21} & w^{(1)}_{31} \\
    w^{(1)}_{12} & w^{(1)}_{22} & w^{(1)}_{32} \\
\end{pmatrix}
\end{align}
$$
:::

위의 식(@eq-forANN )에 따라 각 층(layer)의 신호 전달과정을 구현해 보도록 하겠습니다.


## Layer-by-Layer signaling

3층 신경망의 신호 전달과정은 아래와 같이 Weighted Sum에 기반하며 그 값을 다시 화성화 함수(가령 Sigmoid)를 통하여 노드의 값이 최종 산출됩니다.

또한, 이전층의 값들을 받아 산출된 값은 다시 입력값으로 하여 다음층으로 전달되는 과정을 거쳐 최종적으로 출력층까지 이 과정을 반복하게 됩니다. 이 과정이 신호 전달 또는 Forward propagation 입니다.

::: {#fig-forANN layout-ncol=3}
![input-hidden](/image/ann1.png){#fig-forANN1}

![hidden-hidden](/image/ann2.png){#fig-forANN2}

![hidden-output](/image/ann3.png){#fig-forANN3}

Process of Forward Propagation
:::

**input to hidden**

최초 입력층(0층)의 신호 전달 체계는 입력값(노드)은 2개인 1차원 배열이고 다음의 은닉층(1층)은 노드 3계로 이루어진 1차원 배열입니다. 2개의 노드 값을 받아 3개의 노드로 전달해야 하므로 노드간의 간선은 총 6개($6 = 2 \times 3$)입니다.

입력값 및 편향값을 `a`에 전달하고 `a`값을 활성화 함수(Sigmoid 함수를 사용) `h()`를 이용하여 신호 `z`를 산출하도록 합니다. 결과적으로 은닉층(1층)에 해당하는 3개의 노드의 신호를 확인할 수 있습닏.

``` {python}
# 입력값, 편향, 가중치
X  = np.array([1.0, 0.5])
B1 = np.array([0.1,0.2,0.3])
W1 = np.array([[0.1,0.3,0.5], [0.2,0.4,0.6]])

# Weighted Sum
A1 = np.dot(X, W1) + B1

# Activation Function
Z1 = sigmoid(A1)

print(A1)
print(Z1)
```

**hidden to hidden**

은닉층(1층)이 다시 입력층으로 하여 다음의 은닉층(2층)으로 신호를 전달하도록 해야 합니다. 앞서 진행한 신호 전달 과정과 동일합니다.

다만, 입력 노드가 편향을 포함하여 4개가 다음 층인 2개의 노드로 전달됨에 따라 이전 과정과 달리 간선은 총 8개 입니다. 편향은 2개 간선을 갖고 가중치는 입력 노드별 2개 총 6개로 이루어 집니다.

``` {python}
# 편향, 가중치
B2 = np.array([0.1,0.2])
W2 = np.array([[0.1,0.4], [0.2,0.5], [0.3,0.6]])

# Weighted Sum & Activation Function
A2 = np.dot(Z1, W2) + B2
Z2 = sigmoid(A2)

print(A2)
print(Z2)
```

**hidden to output**

은닉층(2층)이 다시 입력층으로 하여 다음의 출력층(3층)으로 신호를 전달하도록 해야 합니다. 앞서 진행한 신호 전달 과정과 동일합니다.

주의할 것은 노드별 

``` {python}
# 편향, 가중치
B2 = np.array([0.1,0.2])
W2 = np.array([[0.1,0.4], [0.2,0.5], [0.3,0.6]])

# Weighted Sum & Activation Function
A2 = np.dot(Z1, W2) + B2
Z2 = sigmoid(A2)

print(A2)
print(Z2)
```
