# Initializing parameters
---

신경망의 주요 매개변수인 가중치의 초깃값을 어떻게 설정해야 할까요? 이는 학습의 성패를 가르기도 하는 매우 중요한 문제입니다. 기본적으로 알아야 할것은 은닉층으로 연결되는 간선의 가중치는 작은 값(0을 의미하는 것이 아님에 주의)을 갖도록 하고, 무작위적으로 설정되어야 한다는 것입니다.

## Weighting initialization when using Sigmoid functions
**(Distribution of activation values in the hidden layer)**

은닉층의 화성화값은 활성화 함수에서 출력되는 데이터를 말합니다. 스탠퍼드 대학교의 **CS231n**의 수업에서 시그모이드 함수로 구현된 은닉층에 무작위로 생성한 입력데이터를 넣어 출력되는 데이터의 분포를 히스토그램으로 그려보았습니다.

먼저 수업에 사용된 코드는 아래와 같습니다.

``` {python}
#| echo: True
#| eval: False
# coding: utf-8
import numpy as np
import matplotlib.pyplot as plt

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def ReLU(x):
    return np.maximum(0, x)

def tanh(x):
    return np.tanh(x)
    
input_data = np.random.randn(1000, 100)  # 1000개의 데이터
node_num = 100  # 각 은닉층의 노드(뉴런) 수
hidden_layer_size = 5  # 은닉층이 5개
activations = {}  # 이곳에 활성화 결과를 저장

x = input_data

for i in range(hidden_layer_size):
    if i != 0:
        x = activations[i-1]

    # 초깃값을 다양하게 바꿔가며 실험해보자！
    w = np.random.randn(node_num, node_num) * 1 # <1>
    # w = np.random.randn(node_num, node_num) * 0.01
    # w = np.random.randn(node_num, node_num) * np.sqrt(1.0 / node_num)
    # w = np.random.randn(node_num, node_num) * np.sqrt(2.0 / node_num)

    a = np.dot(x, w)

    # 활성화 함수도 바꿔가며 실험해보자！
    z = sigmoid(a) # <2>
    # z = ReLU(a)
    # z = tanh(a)

    activations[i] = z

# 히스토그램 그리기
for i, a in activations.items():
    plt.subplot(1, len(activations), i+1)
    plt.title(str(i+1) + "-layer")
    if i != 0: plt.yticks([], [])
    # plt.xlim(0.1, 1)
    # plt.ylim(0, 7000)
    plt.hist(a.flatten(), 30, range=(0,1))
plt.show()
```
1. 가중치 초깃값 설정 코드
2. 활성화 함수 설정 코드

위 소스코드에서 가중치 초기화를 **표준편차가 1인 정규분포**, **표준편차가 0.01인 정규분포** 그리고 **Xavier**^[Xavier는 이전 노드의 갯수가 $n$개인 경우 표준편차가 $\frac{1}{\sqrt{n}}$인 분포를 사용하는 방식입니다.]의 방법을 사용하여 가중치의 활성화값 분포를 확인해보면 아래와 같습니다.

::: {#fig-techInit0 layout-nrow=3}

![Normmal Distribution with STD = 1](image/fig-techInit1.png){}

![Normmal Distribution with STD = 0.01](image/fig-techInit2.png){}

![Xavier](image/fig-techInit3.png){}

Distribution of activation values by weight initialization methodology
:::

표준편차가 1인 정규분포를 이용한 가중치 초기화시에는 0과 1로 치우치는 모습이 보입니다. 이는 역전파시에 기울기 값을 점점 작아시게 하여 **기울기 소실**(8gradient vanising)의 문제를 야기합니다.

표준편차가 0.01인 정규분포를 이용한 가중치 초기화시에는 0.5에 집중되는 모습이 보입니다. 노드별 값의 차이점이 없어 **표현력 제한**의 문제를 야기합니다.

Xavier의 방법은 모든 노드에 골고루 분포되어 기울기 소실 또는 표현력 제한의 이슈를 해결할 수 있는 것으로 보입니다. 현재 일반적으로 딥러닝의 표준으로 사용됩니다.

::: {.callout-note}
## 층이 깊어질 수록 분포가 고르지 않은 이유

시그모이드 함수의 대칭저은 (x, y) = (0, 0.5)이다. 따라서, 층을 지나갈수록 활성화 값이 고르지 않게 나오는 것입니다. 이를 해결하기 위하여는 원점에 대하여 대칭인 tahn함수를 고려할 수 있습니다.
:::

## Weighting initialization when using ReLU functions

ReLU 함수는 앞서 살펴본 Sigmoid와 달리 비선형함수입니다. 따라서 가중치의 초깃값 설정 방법도 다르지 않을까요? 맞습니다. ReLU에 특회된 초깃값 설정 방법으로 **He 초깃값**이 존재합니다.

이는 Xavier와 비슷하게 이전 노드의 갯수가 n개일 때, 표준편차가 ${\sqrt\frac{2}{n}}$인 정규 분포를 사용합니다. 이는 ReLU가 음의 영역이 0이라서 더 넓게 분포시키기 위해 2배의 계수가 필요하다고 해석할 수 있습니다.

그럼 앞서 Sigmoid와 같이 **표준편차가 0.01인 정규분포**, **Xavier** 그리고 **He초깃값**의 방법을 사용하여 가중치의 활성화값 분포를 확인해보면 아래와 같습니다.

![Distribution of activation values by weight initialization methodology](image/fig-techInit4.png){#fig-techInit4}

표준편차가 0.01인 정규분포를 이용한 가중치 초기화시에는 각 층의 활성화값이 아주 작은값들이 나오게 되어 역전파시 학습이 거의 이루어지지 않는 문제를 갖고 있습니다.

Xavier의 방법은 층이 깊어 질수록 한쪽으로 치우짐이 커지고, 학습시 기울기 소실의 문제를 갖고 있습니다.

He 초깃값은 모든 층에서 활성화 값이 균일하가 분포되어 역전파시에 적절한 학습을 기대할 수 있습니다.

::: {.callout-warning}
## 활성화값의 분포

-   초깃값의 설정은 활성화함수를 거쳐 나오는 출력값 즉, 활성화값의 분포를 결정합니다. 적절한 학습을 위해서는 활성화값이 균일하게 분포되고 기울기 소실 또는 표현력 문제를 야기하지 않아야 합니다.

-   결과적으로 Sigmoid는 Xavier, ReLU는 He초깃값을 활용하여 가중치 초깃값을 설정할 경우 활성화값의 분포를 어느 한쪽에  치우침없이 균일하게 설정할 수 있습니다.
:::