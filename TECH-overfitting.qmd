# Overfitting
---

딥러닝 등 기계학습에서는 **오버피팅**을 항상조심해야 합니다. 훈련 또는 학습에 사용한 데이터에 지나치게 적응되어 다른 문제를 해결할 수 없는 상황을 회피하기 위해서죠.

## Understanding overfitting

오버피팅의 문제를 해결하기 위해서는 어떠한 상황에서 오버피팅이 발생하는지에 대한 이해가 필요합니다.

-   매개변수가 많고 표현력이 높은 모델
-   훈련 데이터가 적은 경우

위의 상황을 살펴보기 위해 7층의 신경망을 사용하여 네트워크의 복잡성을 지나치게 높이고, 60,000개인 MNIST 데이터셋의 훈련데이터 중 아주 적은 300개만 사용한다면 오버피팅이 발생할까요?

MNIST 데이터를 이용하여 훈련데이터와 시험데이터의 정확도를 아래 구현된 코드를 활용하여 비교해보도록 하겠습니다.

``` {python}
# coding: utf-8
import os
import sys

sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정
import numpy as np
import matplotlib.pyplot as plt
from dataset.mnist import load_mnist
from common.multi_layer_net import MultiLayerNet
from common.optimizer import SGD

(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)

# 오버피팅을 재현하기 위해 학습 데이터 수를 줄임
x_train = x_train[:300]
t_train = t_train[:300]

# weight decay（가중치 감쇠） 설정 =======================
weight_decay_lambda = 0 # weight decay를 사용하지 않을 경우
# weight_decay_lambda = 0.1
# ====================================================

network = MultiLayerNet(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100], output_size=10,
                        weight_decay_lambda=weight_decay_lambda)
optimizer = SGD(lr=0.01) # 학습률이 0.01인 SGD로 매개변수 갱신

max_epochs = 201
train_size = x_train.shape[0]
batch_size = 100

train_loss_list = []
train_acc_list = []
test_acc_list = []

iter_per_epoch = max(train_size / batch_size, 1)
epoch_cnt = 0

for i in range(1000000000):
    batch_mask = np.random.choice(train_size, batch_size)
    x_batch = x_train[batch_mask]
    t_batch = t_train[batch_mask]

    grads = network.gradient(x_batch, t_batch)
    optimizer.update(network.params, grads)

    if i % iter_per_epoch == 0:
        train_acc = network.accuracy(x_train, t_train)
        test_acc = network.accuracy(x_test, t_test)
        train_acc_list.append(train_acc)
        test_acc_list.append(test_acc)

        print("epoch:" + str(epoch_cnt) + ", train acc:" + str(train_acc) + ", test acc:" + str(test_acc))

        epoch_cnt += 1
        if epoch_cnt >= max_epochs:
            break


# 그래프 그리기==========
markers = {'train': 'o', 'test': 's'}
x = np.arange(max_epochs)
plt.plot(x, train_acc_list, marker='o', label='train', markevery=10)
plt.plot(x, test_acc_list, marker='s', label='test', markevery=10)
plt.xlabel("epochs")
plt.ylabel("accuracy")
plt.ylim(0, 1.0)
plt.legend(loc='lower right')
plt.show()
```

훈련 데이터를 사용한 정확도는 epoch가 100을 지날 무렴 정확도는 100%에 근접함에 반하여 시험 데이터를 사용한 정확도는 60%~70% 수준에 머무르는 것을 확인할 수 있습니다.

이는 결국 훈련 데이터에만 지나치게 적용(fitting)하여 시험 데이터 등에 대한 범용성을 확보하지 못하였음을 즉, 오버피팅이 발생하였음을 보여줍니다.

## Weight decay

첫번째로 살펴볼 오버피팅을 억제하는 수단으로는 **가중치 감소**(weight decay, $\lambda$)가 있습니다. 이는 학습과정 에서 가장 큰 가중치에 대해서는 그에 상응하는 큰 패널티를 부과하는 방법입니다.^[$L(\textbf{w})+\frac{1}{2}\lambda\textbf{W}^2$]


페널티는 어떻게 산출해야 할까요? 순전파시에 손실함수 값에 $\frac{1}{2}\lambda\textbf{W}^2$를 더하여 역전파시 가중치 업데이트 과정에서 그 마분값인 $\lambda\textbf{W}$를 더하게 되어 가중치가 큰 곳에 더 큰 패널티가 산출됩니다.

정규화의 강도를 조절, 즉, 강중치 감소 정도는 하이퍼파라미터인 $\lambda$로 조절합니다.

$$
\begin{align}
\textbf{L}^* &= \textbf{L}(\textbf{W})+\frac{1}{2}\lambda\textbf{W}^2 \\ \\
\textbf{W} &= \textbf{W} - \eta\frac{\partial{\textbf{L}^*}}{\partial{\textbf{W}}} \\
&= \textbf{W} - \eta\frac{\partial{(\textbf{L}+\frac{1}{2}\lambda\textbf{W}^2)}}{\partial{\textbf{W}}} \\
&= \textbf{W} - \eta(\frac{\partial{\textbf{L}}}{\partial{\textbf{W}}}+\frac{\partial\frac{1}{2}\lambda\textbf{W}^2}{\partial{\textbf{W}}}) \\
&= \textbf{W} - \eta(\frac{\partial{\textbf{L}}}{\partial{\textbf{W}}}+\lambda\textbf{W})
\end{align}
$$

::: {.callout-note}
## L2 Norm을 손실함수 값에 더한다? 그런데 왜 $\sqrt{w_1^2+w_2^2+\cdots+w_i^2}$이지?

-   L2 Norm은 $\sqrt{w_1^2+w_2^2+\cdots+w_i^2}$ 꼴로 나타나느데 왜 딥러닝의 가중치감소에서는 손실함수에 더하는 L2 Norm을 $\frac{1}{2}\lambda\textbf{W}^2$으로 정의되었을까요?
-   먼저 $\frac{1}{2}$항을 살펴보면 이는 수학적 편의를 위한 트릭에 해당합니다. 더해진 값을 역전파시 미분해야 하는데 $\lambda|\textbf{W}|_2^2$을 미분하는 것보다 $\frac{1}{2}\lambda|\textbf{W}|_2^2$을 미분하여  $\lambda\textbf{W}$가 되면 보다 식을 단순하게 만들 수 있기 때문입니다.
-   L2 Norm의 제곱은 어디로 갔을까요? 이는 최적화 과정에서 모델 가중치  $\textbf{W}$를 업데이트할때 제곱근 연산이 필요없기 때문에 불필요한 내용을 생락한 것입니다.
:::

지금까지 살펴 본 내용을 바탕으로 가중치 감소에 따른 효과를 살펴 보도록 하겠습니다. 하이퍼파라미터인 $\lambda$는 0.1로 설정하여 가중치 감소를 적용하였습니다.

``` {python}
#| echo: False
# coding: utf-8
import os
import sys

sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정
import numpy as np
import matplotlib.pyplot as plt
from dataset.mnist import load_mnist
from common.multi_layer_net import MultiLayerNet
from common.optimizer import SGD

(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)

# 오버피팅을 재현하기 위해 학습 데이터 수를 줄임
x_train = x_train[:300]
t_train = t_train[:300]

# weight decay（가중치 감쇠） 설정 =======================
# weight_decay_lambda = 0 # weight decay를 사용하지 않을 경우
weight_decay_lambda = 0.1
# ====================================================

network = MultiLayerNet(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100], output_size=10,
                        weight_decay_lambda=weight_decay_lambda)
optimizer = SGD(lr=0.01) # 학습률이 0.01인 SGD로 매개변수 갱신

max_epochs = 201
train_size = x_train.shape[0]
batch_size = 100

train_loss_list = []
train_acc_list = []
test_acc_list = []

iter_per_epoch = max(train_size / batch_size, 1)
epoch_cnt = 0

for i in range(1000000000):
    batch_mask = np.random.choice(train_size, batch_size)
    x_batch = x_train[batch_mask]
    t_batch = t_train[batch_mask]

    grads = network.gradient(x_batch, t_batch)
    optimizer.update(network.params, grads)

    if i % iter_per_epoch == 0:
        train_acc = network.accuracy(x_train, t_train)
        test_acc = network.accuracy(x_test, t_test)
        train_acc_list.append(train_acc)
        test_acc_list.append(test_acc)

        print("epoch:" + str(epoch_cnt) + ", train acc:" + str(train_acc) + ", test acc:" + str(test_acc))

        epoch_cnt += 1
        if epoch_cnt >= max_epochs:
            break


# 그래프 그리기==========
markers = {'train': 'o', 'test': 's'}
x = np.arange(max_epochs)
plt.plot(x, train_acc_list, marker='o', label='train', markevery=10)
plt.plot(x, test_acc_list, marker='s', label='test', markevery=10)
plt.xlabel("epochs")
plt.ylabel("accuracy")
plt.ylim(0, 1.0)
plt.legend(loc='lower right')
plt.show()
```

여전히 오버피팅이 발생하고 있지만 이전과 비교하여 상당부분 훈련 데이터와 시험 데이터간 정확도의 차이가 줄어든 것을 확인할 수 있습니다. 즉, 오버피팅이 억제된 효과를 확인할 수 있습니다.

## Dropout

오버피팅을 억제하기 위하여 가중치 감소를 사용하였습니다. 구현도 쉽고 효과도 어느정도 확인하였습니다. 그러나 신경망의 모델이 복잡해지면 가중치 감소만으로는 적절한 대으이 어려울 수 있습니다.

이러한 문제를 해결하고자 고안된 기법이 바로 **드롭아웃**입니다. 드롭우웃은 신경망의 각층의 노드를 훈련과정에서 임의로 삭제하면서 신호전달을 차단하는 방법입니다. 다만, 시험 또는 시험과정에서는 모든 노드를 사용합니다.^[주의 할 것은 시험 과정에서 각 노드 또는 뉴런의 출력에 훈련때 삭제 안한 비율을 곱하여 출력을 수행합니다.]

![Concept of Dropout](image/fig-techOver1.png){#fig-techOver1}

위에서 설명한 드롭아웃을 구현하도록 하겠습니다. 먼저 순전파를 담당하는 `forward`메서드에서는 훈련 때(`train_flg=True`)만 잘 계산해두면 시험 때 단순히 데이터를 흘리기만 하면됩니다.^[삭제 안한 비율은 곱하지 않아도 됩니다.]


``` {python}
class Dropout:
  def __init__(self, dropout_ratio=0.5):
    self.dropout_ratio = dropout_ratio
    self.mask = None

  def forward(self, x, train_flg=True):
    if train_flg:
      self.mask = np.random.rand(*x.shape) > self.dropout_ratio # <1>
      return x * self.mask
    else:
      return x * (1.0 - self.dropout_ratio)

  def backward(self, dout):
    return dout * self.mask # <2>
```
1. `self.mask`에 삭제할 뉴런을 `False`로 표시
2. `backward`sms ReLU와 동작원리가 같음

위의 구현코드를 가지고 실험한 7층의 신경망의 학습결과입니다.

![Comparison of results(without Dropout(Left), with Dropout(Right))](image/fig-techOver2.png){#fig-techOver2}

드롭아웃을 적용한 경우 훈련 데이터와 시험 데이터간 정확도의 차이가 확연하게 줄어 들었음을 확인할 수 있습니다. 동시에 표현력도 상당히 개선되었음을 확인할 수 있습니다.

::: {.callout-note}
## Dropout과 앙상블 학습

-   기계학습에서 앙상블 학습(ensemble learning)은 개별적으로 학습시킨 여러 모델의 출력을 평균내어 추론하는 방식입니다.
-   드롭아웃은 무작위로 노드 또는 뉴련을 삭제함으로써 각기 다른 신경망을 학습시킨후 평균을 내어 답하는 것과 비슷합니다.
:::
